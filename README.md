# Literature Review Automation System

Automated pipeline for conducting comprehensive literature reviews in neuromorphic computing research.

## Quick Start

### ğŸŒ Web Dashboard (NEW!)

Launch the web dashboard for a user-friendly interface:

```bash
./run_dashboard.sh
```

Then open http://localhost:8000 in your browser to:
- Upload PDFs
- Monitor job progress in real-time
- View logs and download reports
- Retry failed jobs

See [Dashboard Guide](docs/DASHBOARD_GUIDE.md) for detailed instructions.

### Automated Pipeline (Recommended)

Run the full 5-stage pipeline with a single command:

```bash
python pipeline_orchestrator.py
```

**With logging:**
```bash
python pipeline_orchestrator.py --log-file pipeline.log
```

**With custom configuration:**
```bash
python pipeline_orchestrator.py --config pipeline_config.json
```

**Resume from checkpoint:**
```bash
python pipeline_orchestrator.py --resume
```

**Resume from specific stage:**
```bash
python pipeline_orchestrator.py --resume-from judge
```

### Manual Execution

For step-by-step control, run each stage individually:

```bash
# Stage 1: Initial paper review
python Journal-Reviewer.py

# Stage 2: Judge claims
python Judge.py

# Stage 3: Deep requirements analysis (if rejections exist)
python DeepRequirementsAnalyzer.py
python Judge.py  # Re-judge DRA claims

# Stage 4: Sync to database
python sync_history_to_db.py

# Stage 5: Gap analysis and convergence
python Orchestrator.py
```

## Pipeline Stages

1. **Journal-Reviewer**: Screen papers and extract claims
2. **Judge**: Evaluate claims against requirements
3. **DeepRequirementsAnalyzer (DRA)**: Re-analyze rejected claims (conditional)
4. **Sync**: Update CSV database from version history
5. **Orchestrator**: Identify gaps and drive convergence

## Configuration

Create a `pipeline_config.json` file:

```json
{
  "version": "1.2.0",
  "version_history_path": "review_version_history.json",
  "stage_timeout": 7200,
  "log_level": "INFO",
  "retry_policy": {
    "enabled": true,
    "default_max_attempts": 3,
    "default_backoff_base": 2,
    "default_backoff_max": 60,
    "circuit_breaker_threshold": 3,
    "per_stage": {
      "journal_reviewer": {
        "max_attempts": 5,
        "backoff_base": 2,
        "backoff_max": 120,
        "retryable_patterns": ["timeout", "rate limit", "connection error"]
      }
    }
  }
}
```

### Retry Configuration

The pipeline automatically retries transient failures like network timeouts and rate limits:

**Enable retry (default):**
```json
{
  "retry_policy": {
    "enabled": true,
    "default_max_attempts": 3
  }
}
```

**Disable retry:**
```json
{
  "retry_policy": {
    "enabled": false
  }
}
```

**Custom retry per stage:**
```json
{
  "retry_policy": {
    "per_stage": {
      "journal_reviewer": {
        "max_attempts": 5,
        "backoff_base": 2,
        "backoff_max": 120
      }
    }
  }
}
```

**Retryable errors:**
- Network timeouts and connection errors
- Rate limiting (429, "too many requests")
- Service unavailable (503, 502, 504)
- Temporary failures

**Non-retryable errors:**
- Syntax errors, import errors
- File not found
- Permission denied (401, 403)
- Invalid configuration

## Requirements

**Pipeline:**
```bash
pip install -r requirements-dev.txt
```

**Web Dashboard:**
```bash
pip install -r requirements-dashboard.txt
```

Create a `.env` file with your API key:
```
GEMINI_API_KEY=your_api_key_here
DASHBOARD_API_KEY=your-secure-api-key  # For dashboard authentication
```

## ğŸ“ Repository Structure

```
Literature-Review/
â”œâ”€â”€ docs/                          # ğŸ“š All documentation
â”‚   â”œâ”€â”€ README.md                  # Documentation guide
â”‚   â”œâ”€â”€ DASHBOARD_GUIDE.md         # ğŸŒ Web dashboard guide
â”‚   â”œâ”€â”€ CONSOLIDATED_ROADMAP.md    # â­ Master project roadmap
â”‚   â”œâ”€â”€ architecture/              # System design & refactoring
â”‚   â”œâ”€â”€ guides/                    # Workflow & strategy guides
â”‚   â”œâ”€â”€ status-reports/            # Progress tracking
â”‚   â””â”€â”€ assessments/               # Technical evaluations
â”œâ”€â”€ task-cards/                    # ğŸ“‹ Implementation task cards
â”‚   â”œâ”€â”€ README.md                  # Task cards guide
â”‚   â”œâ”€â”€ agent/                     # Agent improvement tasks
â”‚   â”œâ”€â”€ automation/                # Reliability & error handling
â”‚   â”œâ”€â”€ integration/               # Integration test specs
â”‚   â”œâ”€â”€ e2e/                       # End-to-end test specs
â”‚   â””â”€â”€ evidence-enhancement/      # Evidence quality features
â”œâ”€â”€ reviews/                       # ğŸ” Review documentation
â”‚   â”œâ”€â”€ README.md                  # Reviews guide
â”‚   â”œâ”€â”€ pull-requests/             # PR assessments
â”‚   â”œâ”€â”€ architecture/              # Design reviews
â”‚   â””â”€â”€ third-party/               # External audits
â”œâ”€â”€ literature_review/             # ğŸ Main package code
â”‚   â”œâ”€â”€ analysis/                  # Judge, DRA, Recommendations
â”‚   â”œâ”€â”€ reviewers/                 # Journal & Deep reviewers
â”‚   â”œâ”€â”€ orchestrator.py            # Pipeline coordination
â”‚   â””â”€â”€ utils/                     # Shared utilities
â”œâ”€â”€ webdashboard/                  # ğŸŒ Web dashboard
â”‚   â”œâ”€â”€ app.py                     # FastAPI application
â”‚   â”œâ”€â”€ templates/                 # HTML templates
â”‚   â””â”€â”€ static/                    # CSS, JS, images
â”œâ”€â”€ tests/                         # ğŸ§ª Test suite
â”‚   â”œâ”€â”€ unit/                      # Unit tests
â”‚   â”œâ”€â”€ component/                 # Component tests
â”‚   â”œâ”€â”€ integration/               # Integration tests
â”‚   â”œâ”€â”€ webui/                     # Dashboard tests
â”‚   â””â”€â”€ e2e/                       # End-to-end tests
â””â”€â”€ scripts/                       # ğŸ”§ Utility scripts
```

## Documentation

### ğŸ“– Quick Links

**Getting Started:**
- **[docs/guides/WORKFLOW_EXECUTION_GUIDE.md](docs/guides/WORKFLOW_EXECUTION_GUIDE.md)** - How to run the pipeline
- **[docs/CONSOLIDATED_ROADMAP.md](docs/CONSOLIDATED_ROADMAP.md)** â­ - Complete project overview

**Architecture & Design:**
- **[docs/architecture/ARCHITECTURE_REFACTOR.md](docs/architecture/ARCHITECTURE_REFACTOR.md)** - Current repository structure
- **[docs/architecture/ARCHITECTURE_ANALYSIS.md](docs/architecture/ARCHITECTURE_ANALYSIS.md)** - System architecture

**Testing & Status:**
- **[docs/status-reports/TESTING_STATUS_SUMMARY.md](docs/status-reports/TESTING_STATUS_SUMMARY.md)** - Test coverage
- **[docs/TEST_MODIFICATIONS.md](docs/TEST_MODIFICATIONS.md)** - Enhanced test specifications

**Task Planning:**
- **[task-cards/README.md](task-cards/README.md)** - All implementation tasks (23 cards)
- **[task-cards/evidence-enhancement/](task-cards/evidence-enhancement/)** - Evidence quality features

See **[docs/README.md](docs/README.md)** for complete documentation index.

## Pipeline Orchestrator Features

- âœ… **Automated Execution**: Runs all 5 stages sequentially
- âœ… **Conditional DRA**: Only runs when rejections are detected
- âœ… **Progress Logging**: Timestamps and status for each stage
- âœ… **Error Handling**: Halts on failure with clear error messages
- âœ… **Configurable**: Customizable timeouts and paths
- âœ… **Checkpoint/Resume**: Resume from interruption points
- âœ… **Automatic Retry**: Retry transient failures with exponential backoff
- âœ… **Circuit Breaker**: Prevents infinite retry loops
- âœ… **Retry History**: Track all retry attempts in checkpoint file

### Checkpoint & Resume

The pipeline creates a `pipeline_checkpoint.json` file to track progress. If a pipeline fails, you can resume from the last successful stage:

```bash
# Resume from last checkpoint
python pipeline_orchestrator.py --resume

# Resume from specific stage
python pipeline_orchestrator.py --resume-from sync
```

**View checkpoint status:**
```bash
cat pipeline_checkpoint.json | jq '.stages'
```

**View retry history:**
```bash
cat pipeline_checkpoint.json | jq '.stages.journal_reviewer.retry_history'
```

### Error Recovery

The pipeline automatically retries transient failures:

1. **Network Timeout** â†’ Retry with exponential backoff
2. **Rate Limit** â†’ Wait and retry with increasing delays
3. **Syntax Error** â†’ Fail immediately (no retry)
4. **Circuit Breaker** â†’ Stop after 3 consecutive failures

**Example retry flow:**
- Attempt 1: Fails with "Connection timeout" â†’ Wait 2s, retry
- Attempt 2: Fails with "Rate limit" â†’ Wait 4s, retry
- Attempt 3: Succeeds â†’ Continue to next stage

## Testing

Run the test suite:

```bash
pytest
```

Run specific test categories:

```bash
pytest -m unit          # Unit tests only
pytest -m integration   # Integration tests only
```

## License

See [LICENSE](LICENSE) file for details.
