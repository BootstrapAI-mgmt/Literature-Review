"""Integration tests for Version History â†’ CSV sync."""

import pytest
import json
import csv
import os
import pandas as pd
from typing import Dict, List

import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# Import sync functionality (if refactored) or test script execution


class TestVersionHistorySync:
    """Test sync from version history to CSV database."""
    
    @pytest.mark.integration
    def test_sync_approved_claims_to_csv(self, temp_workspace, test_data_generator):
        """Test that sync copies only approved claims to CSV."""
        
        # Setup: Create version history with mixed statuses
        version_history = {
            "paper1.pdf": [
                {
                    'timestamp': '2025-11-10T10:00:00',
                    'review': {
                        'FILENAME': 'paper1.pdf',
                        'TITLE': 'Test Paper 1',
                        'PUBLICATION_YEAR': 2024,
                        'Requirement(s)': [
                            {
                                'claim_id': 'approved_claim_1',
                                'status': 'approved',
                                'pillar': 'Pillar 1',
                                'sub_requirement': 'SR 1.1',
                                'evidence': 'Approved evidence',
                                'page_number': 1
                            },
                            {
                                'claim_id': 'rejected_claim_1',
                                'status': 'rejected',
                                'pillar': 'Pillar 1',
                                'sub_requirement': 'SR 1.2',
                                'evidence': 'Rejected evidence',
                                'page_number': 2
                            },
                            {
                                'claim_id': 'pending_claim_1',
                                'status': 'pending_judge_review',
                                'pillar': 'Pillar 1',
                                'sub_requirement': 'SR 1.3',
                                'evidence': 'Pending evidence',
                                'page_number': 3
                            }
                        ]
                    }
                }
            ],
            "paper2.pdf": [
                {
                    'timestamp': '2025-11-10T11:00:00',
                    'review': {
                        'FILENAME': 'paper2.pdf',
                        'TITLE': 'Test Paper 2',
                        'PUBLICATION_YEAR': 2025,
                        'Requirement(s)': [
                            {
                                'claim_id': 'approved_claim_2',
                                'status': 'approved',
                                'pillar': 'Pillar 2',
                                'sub_requirement': 'SR 2.1',
                                'evidence': 'Approved evidence 2',
                                'page_number': 5
                            }
                        ]
                    }
                }
            ]
        }
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(version_history, f, indent=2)
        
        csv_file = temp_workspace / "neuromorphic-research_database.csv"
        
        # Execute: Perform sync (simplified sync logic)
        # In real implementation, this would call sync_history_to_db.py
        synced_papers = []
        
        for filename, versions in version_history.items():
            if not versions:
                continue
            
            latest = versions[-1]['review']
            
            # Extract approved claims
            approved_claims = [
                c for c in latest.get('Requirement(s)', [])
                if c.get('status') == 'approved'
            ]
            
            if approved_claims:
                paper_entry = {
                    'FILENAME': latest.get('FILENAME', filename),
                    'TITLE': latest.get('TITLE', ''),
                    'PUBLICATION_YEAR': latest.get('PUBLICATION_YEAR', ''),
                    'Requirement(s)': json.dumps(approved_claims)
                }
                synced_papers.append(paper_entry)
        
        # Write to CSV
        if synced_papers:
            fieldnames = ['FILENAME', 'TITLE', 'PUBLICATION_YEAR', 'Requirement(s)']
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
                writer.writeheader()
                writer.writerows(synced_papers)
        
        # Assert: Verify sync results
        assert csv_file.exists()
        
        # Read CSV
        df = pd.read_csv(csv_file)
        
        # Check row count (should be 2 papers)
        assert len(df) == 2
        
        # Check that only approved claims are present
        for _, row in df.iterrows():
            claims = json.loads(row['Requirement(s)'])
            for claim in claims:
                assert claim['status'] == 'approved'
        
        # Verify specific counts
        paper1_row = df[df['FILENAME'] == 'paper1.pdf'].iloc[0]
        paper1_claims = json.loads(paper1_row['Requirement(s)'])
        assert len(paper1_claims) == 1  # Only 1 approved out of 3
        
        paper2_row = df[df['FILENAME'] == 'paper2.pdf'].iloc[0]
        paper2_claims = json.loads(paper2_row['Requirement(s)'])
        assert len(paper2_claims) == 1  # 1 approved claim
    
    @pytest.mark.integration
    def test_sync_handles_empty_version_history(self, temp_workspace):
        """Test sync gracefully handles empty version history."""
        
        # Setup: Empty version history
        version_history = {}
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(version_history, f, indent=2)
        
        csv_file = temp_workspace / "neuromorphic-research_database.csv"
        
        # Execute: Sync should create empty or minimal CSV
        synced_papers = []
        for filename, versions in version_history.items():
            pass  # No papers to sync
        
        # Should not create CSV or create empty CSV
        # For this test, we verify no crash occurs
        assert True  # Sync completed without error
    
    @pytest.mark.integration
    def test_sync_preserves_claim_fields(self, temp_workspace):
        """Test that all claim fields are preserved in sync."""
        
        # Setup: Version history with extended claim fields
        version_history = {
            "detailed_paper.pdf": [
                {
                    'timestamp': '2025-11-10T12:00:00',
                    'review': {
                        'FILENAME': 'detailed_paper.pdf',
                        'TITLE': 'Detailed Test',
                        'Requirement(s)': [
                            {
                                'claim_id': 'detailed_claim',
                                'status': 'approved',
                                'pillar': 'Pillar 1',
                                'sub_requirement': 'SR 1.1',
                                'evidence': 'Detailed evidence',
                                'page_number': 10,
                                'reviewer_confidence': 0.95,
                                'review_timestamp': '2025-11-10T09:00:00',
                                'source': 'deep_coverage',
                                'judge_notes': 'Approved. Excellent evidence.',
                                'judge_timestamp': '2025-11-10T10:00:00'
                            }
                        ]
                    }
                }
            ]
        }
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(version_history, f, indent=2)
        
        csv_file = temp_workspace / "neuromorphic-research_database.csv"
        
        # Execute sync
        synced_papers = []
        for filename, versions in version_history.items():
            latest = versions[-1]['review']
            approved_claims = [
                c for c in latest.get('Requirement(s)', [])
                if c.get('status') == 'approved'
            ]
            if approved_claims:
                paper_entry = {
                    'FILENAME': latest['FILENAME'],
                    'TITLE': latest['TITLE'],
                    'Requirement(s)': json.dumps(approved_claims)
                }
                synced_papers.append(paper_entry)
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(
                f,
                fieldnames=['FILENAME', 'TITLE', 'Requirement(s)'],
                quoting=csv.QUOTE_ALL
            )
            writer.writeheader()
            writer.writerows(synced_papers)
        
        # Assert: Verify all fields preserved
        df = pd.read_csv(csv_file)
        row = df.iloc[0]
        claims = json.loads(row['Requirement(s)'])
        claim = claims[0]
        
        # Check extended fields present
        assert 'claim_id' in claim
        assert 'reviewer_confidence' in claim
        assert claim['reviewer_confidence'] == 0.95
        assert 'review_timestamp' in claim
        assert 'source' in claim
        assert claim['source'] == 'deep_coverage'
        assert 'judge_notes' in claim
        assert 'judge_timestamp' in claim
