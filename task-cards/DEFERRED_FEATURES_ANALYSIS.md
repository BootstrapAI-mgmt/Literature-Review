# Deferred Features & Follow-Up Tasks

**Analysis Date:** November 17, 2025  
**Last Updated:** November 17, 2025 (after PR #47)  
**Scope:** Last 12 merged PRs (#36-#47)  
**Purpose:** Identify incomplete features, deferred improvements, and known limitations requiring follow-up task cards

---

## üìã Phase 4: Results Visualization (PR #39)

### ‚úÖ Completed
- Task 4.1: Results Retrieval API
- Task 4.2: Results Viewer UI  
- Task 4.5: Test Results Visualization

### ‚ùå Deferred Tasks

#### **Task 4.3: Results Comparison View**
**Status:** NOT IMPLEMENTED  
**Priority:** Medium  
**Effort:** 5 hours  
**Description:** Side-by-side comparison of gap analysis runs (before/after incremental updates)

**Requirements:**
- Compare gap analysis metrics across multiple job runs
- Highlight what changed (papers added, completeness improved)
- Visualize improvement trends over time
- Useful for tracking research progress

**Acceptance Criteria:**
- [ ] UI component for selecting two jobs to compare
- [ ] Side-by-side comparison table showing delta in completeness
- [ ] Papers added/removed differential view
- [ ] Trend chart showing completeness improvement over time

---

#### **Task 4.4: Results Summary Cards**
**Status:** NOT IMPLEMENTED  
**Priority:** Medium  
**Effort:** 3 hours  
**Description:** Quick-view cards for key metrics without opening full results

**Requirements:**
- Dashboard cards showing high-level metrics per job
- Gap coverage percentage
- Critical gaps count
- Paper count
- Recommendations preview

**Acceptance Criteria:**
- [ ] Summary cards on job list page
- [ ] Click to expand full results modal
- [ ] Metric badges (e.g., "85% covered", "3 critical gaps")
- [ ] Color-coded status indicators

---

## üìã Phase 5: Interactive Prompts ‚úÖ COMPLETE (PR #46)

### ‚úÖ All Tasks COMPLETED
**Status:** ENTIRE PHASE IMPLEMENTED  
**Completed:** November 17, 2025 (PR #46)  
**Total Effort:** 21 hours  
**Description:** Full 1:1 parity with terminal experience achieved - handle user prompts via WebSocket

**Implementation Summary:**
- ‚úÖ Task 5.1: Prompt Handler System (`webdashboard/prompt_handler.py` - 139 lines)
- ‚úÖ Task 5.2: Integrate Prompts into Orchestrator (`OrchestratorConfig.prompt_callback` + 116 lines)
- ‚úÖ Task 5.3: Prompt UI Components (Bootstrap modal + 255 lines in `index.html`)
- ‚úÖ Task 5.4: Integrate Prompt Handler with JobRunner (wired via `app.py` + 28 lines)
- ‚úÖ Task 5.5: Test Interactive Prompts (10 tests: 6 passing, 4 skipped due to env)

**Key Features Delivered:**
- Async prompt lifecycle management with `PromptHandler` class
- 5-minute default timeout with countdown timer in UI
- Concurrent job support (multiple jobs can prompt simultaneously via `prompt_job_ids` tracking)
- Full backward compatibility (terminal mode still works via `prompt_callback=None`)
- Interactive modal UI with radio buttons for pillar selection, run mode, and continue prompts
- WebSocket-based bidirectional communication (prompt broadcast ‚Üí user response ‚Üí orchestrator resume)
- Proper error handling (404 for invalid prompt ID, cleanup on timeout, validation before orchestrator)

**Code Quality Highlights:**
- Clean async/await patterns throughout
- Type hints on all functions
- Comprehensive docstrings
- No new external dependencies (uses built-in `asyncio`)
- Timezone-aware datetime handling (`datetime.now(timezone.utc)`)
- CodeQL: 0 security alerts

**Result:** Dashboard now has complete feature parity with terminal experience. Users can control entire job lifecycle via web interface without terminal interaction.

### üî∂ Phase 5 Known Limitations & Deferred Items (PR #46)

**Review Source:** PR #46 Review by BootstrapAI-mgmt  
**Review Date:** November 17, 2025  
**Overall Verdict:** APPROVED ‚úÖ with 6 deferred enhancements

---

#### **1. ‚ùå Pytest-Asyncio Not in Requirements** (CRITICAL)
**Status:** MISSING  
**Priority:** üî¥ High  
**Effort:** 1 minute  
**Identified In:** PR #46 Review - Configuration & Dependencies Section

**Issue:**
- `pytest-asyncio` installed manually during PR #46 testing
- Not added to `requirements-dev.txt`
- CI/CD or fresh environments will fail async tests
- Current workaround: manual `pip install pytest-asyncio`

**Impact:**
- CI/CD pipeline failures if async tests run
- Developer onboarding friction (missing dependency error)
- Test suite incomplete in clean environments

**Fix:**
```bash
echo "pytest-asyncio>=0.21.0" >> requirements-dev.txt
```

**Acceptance Criteria:**
- [x] Package installed (done in PR #46)
- [ ] Added to `requirements-dev.txt`
- [ ] Verified in CI pipeline
- [ ] `pytest.ini` configured with `asyncio_mode = auto` (‚úÖ already done in PR #46)

**Task Card:** Create `ENHANCE-P5-1: Add pytest-asyncio to requirements-dev.txt`

---

#### **2. üî∂ Limited Prompt Types Implemented**
**Status:** PARTIAL IMPLEMENTATION  
**Priority:** üü° Medium  
**Effort:** 2 hours  
**Identified In:** PR #46 Review - Deferred Items Section #2

**Description:** Only `pillar_selection` fully implemented, other prompt types have UI but no orchestrator integration

**Current State:**
- ‚úÖ `pillar_selection`: Fully implemented (orchestrator + UI + tests)
- ‚ö†Ô∏è `run_mode`: UI implemented (`renderRunModePrompt()`) but orchestrator doesn't call it
- ‚ö†Ô∏è `continue`: UI implemented (`renderContinuePrompt()`) but no orchestrator usage
- ‚ùå `threshold_selection`: Not implemented
- ‚ùå `convergence_confirmation`: Not implemented

**Use Case:**
- User starts job without `run_mode` in config ‚Üí dashboard should prompt for ONCE vs DEEP_LOOP
- Deep loop iteration finishes ‚Üí prompt "Continue deep review loop? Yes/No"
- Gap analysis starts ‚Üí prompt for custom convergence threshold (e.g., 5.0 vs 10.0)

**Proposed Enhancement:**
1. Add `run_mode` prompt when user starts job without mode configured
2. Use `continue` prompt in deep loop workflow (replace hardcoded logic)
3. Add threshold selection for custom gap analysis thresholds
4. Convergence confirmation before exiting deep loop

**Implementation Plan:**
```python
# In orchestrator.py main()
if config and not config.run_mode:
    # Prompt for run mode if not provided
    run_mode = await config.prompt_callback(
        prompt_type="run_mode",
        prompt_data={"message": "Select analysis mode", "default": "ONCE"}
    )
```

**Acceptance Criteria:**
- [ ] `run_mode` prompt integrated with orchestrator (when mode not in config)
- [ ] `continue` prompt used in deep loop workflow (after each iteration)
- [ ] Optional: threshold selection prompt before gap analysis
- [ ] Optional: convergence confirmation before exiting loop
- [ ] Tests updated to cover new prompt types

**Task Card:** Create `ENHANCE-P5-2: Integrate run_mode and continue prompts with orchestrator`

---

#### **3. üî∂ No Prompt History/Replay**
**Status:** NOT IMPLEMENTED  
**Priority:** üü¢ Low  
**Effort:** 3 hours  
**Identified In:** PR #46 Review - Deferred Items Section #3

**Description:** Past prompts and responses not saved to job metadata

**Use Case:**
- User wants to see what they selected for historical job (e.g., "Did I run this with ALL pillars or just one?")
- Audit trail of user decisions for compliance/reproducibility
- Replay job with same choices (job re-run with identical config)

**Current Limitation:**
- Prompts only exist in memory during job execution
- Once job completes, no record of what user selected
- No way to audit or reproduce interactive job runs

**Proposed Solution:**
```python
# In PromptHandler.submit_response()
job_data['prompts'] = job_data.get('prompts', [])
job_data['prompts'].append({
    'prompt_id': prompt_id,
    'type': prompt_type,
    'response': response,
    'timestamp': datetime.now(timezone.utc).isoformat(),
    'timeout': timeout_seconds,
    'timed_out': False  # or True if timeout occurred
})
```

**UI Integration:**
- Display prompt history in job details modal
- Section: "User Decisions"
- Format: "2025-11-17 10:30:00 - Pillar Selection: ALL (5min timeout)"

**Acceptance Criteria:**
- [ ] Prompt responses saved to job JSON
- [ ] Display in job details modal (new "User Decisions" section)
- [ ] Filter/search by prompt type
- [ ] Export prompt history with job results

**Task Card:** Create `ENHANCE-P5-3: Add prompt history to job metadata`

---

#### **4. üî∂ No Multi-Select for Pillars**
**Status:** LIMITATION  
**Priority:** üü¢ Low  
**Effort:** 2 hours  
**Identified In:** PR #46 Review - Deferred Items Section #4

**Description:** Can only select one pillar or ALL, not subset

**Current Behavior:**
- Radio buttons: ALL, DEEP, individual pillars, NONE
- Terminal mode allows comma-separated: "1,3,5" ‚Üí [pillar1, pillar3, pillar5]
- Dashboard doesn't support multi-select (only single pillar or ALL)

**Use Case:**
- User wants to analyze specific subset: "I only care about pillars 1, 3, and 5 today"
- Avoids running full "ALL" analysis when only testing a few pillars
- Matches terminal mode flexibility

**Proposed Enhancement:**
1. Replace radio buttons with checkboxes for pillar selection
2. Add "Select All" / "Clear All" convenience buttons
3. Parse multi-select responses: `["P1: Pillar 1", "P3: Pillar 3", "P5: Pillar 5"]`
4. Maintain backward compatibility: "ALL" checkbox selects all, "NONE" deselects all

**UI Mockup:**
```html
<div class="form-check">
    <input type="checkbox" id="select-all" onclick="toggleAll()">
    <label>Select All</label>
</div>
<hr>
<div class="form-check">
    <input type="checkbox" name="pillar" value="P1: Pillar 1">
    <label>1. Pillar 1</label>
</div>
<!-- ... more pillars ... -->
<button onclick="clearAll()">Clear All</button>
```

**Acceptance Criteria:**
- [ ] Checkbox UI for pillar selection (replace radio buttons)
- [ ] Multi-select response parsing (array of pillar names)
- [ ] Update orchestrator to handle pillar subsets
- [ ] "Select All" / "Clear All" buttons
- [ ] Validation: at least one pillar selected (unless NONE chosen)

**Task Card:** Create `ENHANCE-P5-4: Add multi-select pillar support to interactive prompts`

---

#### **5. üî∂ Timeout Not Configurable Per-Prompt**
**Status:** HARDCODED  
**Priority:** üü¢ Low  
**Effort:** 1 hour  
**Identified In:** PR #46 Review - Deferred Items Section #5

**Description:** All prompts use 300s (5-minute) timeout

**Current Limitation:**
```python
# In job_runner.py
response = await prompt_handler.request_user_input(
    job_id=job_id,
    prompt_type=prompt_type,
    prompt_data=prompt_data,
    timeout_seconds=300  # Hardcoded 5 minutes for all prompts
)
```

**Proposed Enhancement:**
- Allow per-prompt timeout configuration
- Different timeouts for different prompt types:
  - `pillar_selection`: 300s (complex choice, keep 5min)
  - `run_mode`: 120s (2min - simpler binary choice)
  - `continue`: 60s (1min - simple yes/no)
  - `threshold_selection`: 180s (3min - numeric input)
- Configurable via `pipeline_config.json`

**Configuration Example:**
```json
{
  "prompts": {
    "default_timeout": 300,
    "timeouts": {
      "pillar_selection": 300,
      "run_mode": 120,
      "continue": 60,
      "threshold_selection": 180
    }
  }
}
```

**Implementation:**
```python
# In PromptHandler
def get_timeout(self, prompt_type: str) -> int:
    """Get timeout for prompt type from config"""
    config = load_config()
    timeouts = config.get('prompts', {}).get('timeouts', {})
    return timeouts.get(prompt_type, self.default_timeout)
```

**Acceptance Criteria:**
- [ ] `timeout` parameter configurable in `pipeline_config.json`
- [ ] Per-prompt-type timeout configuration
- [ ] UI displays appropriate countdown (reads from `prompt_data.timeout_seconds`)
- [ ] Fallback to default (300s) if not configured
- [ ] Tests verify different timeouts work correctly

**Task Card:** Create `ENHANCE-P5-5: Make prompt timeout configurable per prompt type`

---

#### **6. üî∂ Missing Interactive Prompts Documentation**
**Status:** NOT DOCUMENTED  
**Priority:** üü° Medium  
**Effort:** 2 hours  
**Identified In:** PR #46 Review - Documentation Section

**Description:** Phase 5 functionality not documented for end users

**Missing Documentation:**
- ‚ùå `docs/INTERACTIVE_PROMPTS_GUIDE.md` (user-facing guide)
- ‚ùå No update to `DASHBOARD_GUIDE.md` mentioning prompt feature
- ‚ùå No screenshots of prompt modal in PR #46

**Impact:**
- Users don't know how to use interactive prompts
- No documentation on timeout behavior (what happens when timeout expires?)
- No comparison between terminal mode and dashboard mode workflows

**Proposed Content:**

1. **User Guide** (`docs/INTERACTIVE_PROMPTS_GUIDE.md`):
   - **Overview:** What are interactive prompts?
   - **Available Prompt Types:**
     - Pillar Selection (choose which pillars to analyze)
     - Run Mode (ONCE vs DEEP_LOOP)
     - Continue (yes/no prompts in deep loop)
   - **Timeout Behavior:**
     - Default 5-minute timeout
     - Visual countdown in modal
     - Auto-cancel on timeout
   - **What Happens On:**
     - User selects option ‚Üí job continues
     - Timeout expires ‚Üí job cancelled, no analysis run
     - User clicks "Cancel Job" ‚Üí same as timeout
   - **Comparison Table:**
     | Feature | Terminal Mode | Dashboard Mode |
     |---------|--------------|----------------|
     | Pillar Selection | `input()` prompt | Modal dialog |
     | Timeout | None (blocks forever) | 5min (auto-cancel) |
     | Concurrent Jobs | N/A (single process) | Yes (per-job prompts) |

2. **Dashboard Guide Update** (`docs/DASHBOARD_GUIDE.md`):
   - Add section: "## Interactive Job Control"
   - Screenshot of prompt modal (capture from running dashboard)
   - Step-by-step: "How to start a job that requires user input"
   - Example workflow: Start job without pillar selection ‚Üí modal appears ‚Üí select ALL ‚Üí job runs

**Acceptance Criteria:**
- [ ] Create `INTERACTIVE_PROMPTS_GUIDE.md`
- [ ] Update `DASHBOARD_GUIDE.md` with prompt section
- [ ] Add screenshots to documentation (at least 2: pillar selection modal, timeout countdown)
- [ ] Update `README.md` to link to new guide
- [ ] Review by another team member for clarity

**Task Card:** Create `ENHANCE-P5-6: Document interactive prompts feature`

---

## üìã Phase 2: Input Handling (PR #36)

### üî∂ Known Limitations

#### **Duplicate Detection Across Uploads**
**Status:** NOT IMPLEMENTED  
**Priority:** Low  
**Effort:** 3 hours  
**Description:** Current deduplication only works within single upload batch

**Impact:** Users can upload same PDF multiple times in different batches, creating duplicates

**Solution:**
- Check existing database for duplicates before inserting
- Use PDF hash or title matching
- Show "already exists" warning to user

**Acceptance Criteria:**
- [ ] Cross-batch duplicate detection
- [ ] User warning on duplicate upload attempt
- [ ] Option to skip or force re-upload

---

#### **PDF Metadata Extraction Improvements**
**Status:** HEURISTIC ONLY  
**Priority:** Low  
**Effort:** 5 hours  
**Description:** Current metadata extraction is basic, may miss fields

**Current Issues:**
- Title extraction sometimes gets first line instead of actual title
- Abstract detection is heuristic
- Author parsing unreliable for some formats
- No DOI extraction

**Proposed Improvements:**
- Integrate PyMuPDF or pdfplumber for better text extraction
- Add DOI pattern matching
- Improve abstract boundary detection
- Better author name parsing

---

## üìã Phase 3: Progress Monitoring (PR #38)

### üî∂ Known Limitations

#### **ETA Accuracy**
**Status:** IMPLEMENTED BUT INACCURATE  
**Priority:** Low  
**Effort:** 2 hours  
**Description:** ETA is estimate only, not accurate for first run

**Current Issue:**
- Uses fixed time estimates per stage
- Doesn't account for paper count or complexity
- First run has no historical data

**Proposed Improvements:**
- Track actual stage durations
- Save to database for future ETA calibration
- Factor in paper count (e.g., 2min/paper for deep review)
- Show confidence interval

---

#### **No Progress Replay for Historical Jobs**
**Status:** NOT IMPLEMENTED  
**Priority:** Low  
**Effort:** 4 hours  
**Description:** Progress tracking only works for active jobs

**Use Case:**
- User wants to see how long each stage took for completed job
- Useful for debugging slow runs

**Proposed Solution:**
- Save stage timestamps to job metadata
- Reconstruct progress timeline from timestamps
- Display as readonly progress visualization

---

## üìã Enhancement Wave 3

### üî∂ Smart Deduplication Enhancements ‚úÖ MOSTLY COMPLETE (PR #44, #47)

#### ~~**Model Download Firewall Block**~~ ‚úÖ DOCUMENTED
**Status:** DOCUMENTED IN PR #47  
**Priority:** Medium ‚Üí CLOSED  
**Effort:** 2 hours ‚Üí 0 hours remaining  
**Resolution:** Documented as known limitation with workaround

**PR #47 Actions:**
- Documented firewall blocking issue in PR description
- Tests properly skip when model unavailable (`is_model_available()` check)
- Clear error messaging implemented
- Deployment documentation includes model caching requirement

**Outcome:** Issue acknowledged and documented. Model caching handled in Docker images. No further action needed.

---

#### ~~**Performance for Large Datasets**~~ ‚úÖ PARTIALLY RESOLVED
**Status:** IMPROVED IN PR #47  
**Priority:** Medium ‚Üí Low  
**Effort:** 6 hours ‚Üí 2 hours remaining  
**Resolution:** Cross-batch detection added, performance benchmarks created

**PR #47 Improvements:**
- ‚úÖ Cross-batch duplicate detection (progressive comparison algorithm)
- ‚úÖ Performance benchmarks added (`tests/performance/test_dedup_performance.py`)
- ‚úÖ Overhead measured: ~10-15% for cross-batch detection
- ‚úÖ Batch processing optimized for memory efficiency

**Remaining Work (2 hours):**
- FAISS/Annoy integration for datasets >500 papers (future optimization)
- Early termination for obviously different papers

**Current Performance:**
- 10 papers: <5s
- 50 papers: <30s
- 100 papers (batch): <120s
- Cross-batch overhead: ~10-15%

---

#### **NEW: Full Metadata Preservation** ‚úÖ COMPLETE
**Status:** IMPLEMENTED IN PR #47  
**Priority:** Medium  
**Effort:** 3 hours (not previously tracked)  
**Description:** Complete metadata from duplicate papers now preserved

**Features Delivered:**
- `duplicate_versions` field with full metadata from merged papers
  - Original metadata dict
  - Title, abstract
  - Similarity score
  - Merge timestamp
- Backward compatible: legacy `duplicates` field maintained
- New test: `test_full_metadata_preservation()` validates retention

**Data Structure:**
```json
{
  "duplicate_versions": [{
    "filename": "paper2.json",
    "similarity_score": 0.95,
    "metadata": {"authors": [...], "year": 2023},
    "merged_at": "2025-11-17T10:30:00"
  }]
}
```

---

#### **NEW: Pipeline Integration** ‚úÖ COMPLETE
**Status:** IMPLEMENTED IN PR #47  
**Priority:** Medium  
**Effort:** 2 hours (not previously tracked)  
**Description:** Optional integration with pipeline orchestrator

**Features Delivered:**
- Configuration-based integration in `pipeline_config.json`
- Three integration approaches documented:
  1. Config-based (automatic)
  2. Manual CLI (scripts/deduplicate_papers.py)
  3. Programmatic (Python API)
- Disabled by default (no breaking changes)
- Complete documentation in `docs/smart_deduplication.md` (+159 lines)

**Configuration Options:**
```json
{
  "deduplication": {
    "enabled": false,
    "threshold": 0.90,
    "batch_size": 50,
    "run_before_gap_analysis": true,
    "output_file": "review_log_deduped.json"
  }
}
```

---

### üî∂ Evidence Decay Tracker Enhancements (PR #45)

#### **Future: Integration with Gap Analysis Scoring**
**Status:** TRACKING ONLY, NOT INTEGRATED  
**Priority:** Medium  
**Effort:** 4 hours  
**Description:** Decay weights calculated but not used in gap scoring

**Current Behavior:**
- Generates `evidence_decay.json` report
- Flags stale requirements
- Does NOT affect gap analysis completeness scores

**Proposed Integration:**
- Modify gap analysis to use `freshness_score` instead of raw alignment
- Penalize requirements with old evidence
- Boost requirements with recent evidence
- Configurable via `pipeline_config.json`

**Acceptance Criteria:**
- [ ] Gap analysis uses decay-weighted scores
- [ ] Config option `evidence_decay.weight_in_gap_analysis`
- [ ] A/B comparison showing impact
- [ ] Documentation on when to enable

---

#### **Field-Specific Half-Life Presets**
**Status:** MANUAL CONFIGURATION ONLY  
**Priority:** Low  
**Effort:** 2 hours  
**Description:** Users must manually set half-life, no presets

**Proposed Presets:**
```python
FIELD_PRESETS = {
    "AI/ML": 3.0,
    "Software Engineering": 5.0,
    "Computer Science": 7.0,
    "Mathematics": 10.0,
    "Medicine": 5.0,
    "Biology": 6.0,
    "Physics": 8.0
}
```

**Acceptance Criteria:**
- [ ] Dropdown in UI to select research field
- [ ] Auto-set half-life based on field
- [ ] Option to override with custom value

---

### üî∂ ROI Search Optimizer Enhancements (PR #43)

#### **Dynamic Priority Adjustment**
**Status:** STATIC ROI CALCULATION  
**Priority:** Low  
**Effort:** 3 hours  
**Description:** ROI calculated once, doesn't update after searches complete

**Proposed Enhancement:**
- Recalculate ROI after each search batch
- Deprioritize searches that found sufficient papers
- Boost searches for gaps still critical
- Adaptive prioritization

---

#### **Cost-Aware Search Ordering**
**Status:** NO COST CONSIDERATION  
**Priority:** Low  
**Effort:** 2 hours  
**Description:** ROI doesn't account for API costs

**Proposed Enhancement:**
- Factor in search cost (API calls √ó rate)
- Prioritize high-ROI, low-cost searches first
- Budget-constrained search planning
- Show cost estimate per search

---

## üìã Dashboard Core (PR #36, #37, #38, #39)

### üî∂ v1.0 Known Limitations

#### **File-Based Storage**
**Status:** NOT SUITABLE FOR HIGH SCALE  
**Priority:** Low (OK for v1)  
**Effort:** 40+ hours (major refactor)  
**Description:** Jobs stored as JSON files, not scalable

**Current Limits:**
- ~100 jobs max before performance degrades
- No transactions (risk of corruption)
- No concurrent access safety
- Manual cleanup required

**v2.0 Migration Path:**
- PostgreSQL or SQLite for job metadata
- S3 or blob storage for output files
- Indexed queries for fast job listing
- Atomic job updates

---

#### **Single Server Architecture**
**Status:** NO HORIZONTAL SCALING  
**Priority:** Low (OK for v1)  
**Effort:** 50+ hours  
**Description:** Cannot scale beyond single server

**v2.0 Scaling Path:**
- Queue-based job distribution (Celery/RabbitMQ)
- Multiple worker nodes
- Shared state via Redis
- Load balancer

---

#### **Basic Authentication**
**Status:** API KEY ONLY  
**Priority:** Low (OK for v1)  
**Effort:** 20 hours  
**Description:** No user management, role-based access, or audit logs

**v2.0 Auth Path:**
- User accounts with OAuth2
- Role-based permissions
- Audit logging
- Team/organization support

---

## üìã Integration Testing Gaps

### ‚úÖ Partially Complete

#### **Interactive Prompt Testing** ‚úÖ COMPLETE (PR #46)
**Status:** IMPLEMENTED  
**Completed:** November 17, 2025  
**Effort:** 4 hours  
**Description:** Comprehensive async prompt testing added

**Test Coverage:**
- ‚úÖ Basic request/response cycle
- ‚úÖ Timeout handling (5-minute default)
- ‚úÖ Invalid response handling
- ‚úÖ Job-specific pending prompts
- ‚úÖ Cleanup on timeout
- ‚úÖ Concurrent prompt support (multiple jobs)
- ‚è≠Ô∏è 4 orchestrator integration tests (skipped due to environment dependencies - expected)

**Test File:** `tests/integration/test_interactive_prompts.py` (277 lines, 10 tests)
**Coverage:** 47.83% of `prompt_handler.py` (WebSocket broadcast code not testable without live connection manager)

---

### ‚ùå Missing Test Coverage

#### **End-to-End Dashboard Tests**
**Status:** PARTIALLY IMPLEMENTED  
**Priority:** Medium  
**Effort:** 4 hours (reduced from 8 hours - interactive prompt tests already complete)  
**Description:** Need full workflow tests from upload ‚Üí job ‚Üí results

**Remaining Tests Needed:**
- Upload PDFs ‚Üí verify database
- Start job ‚Üí monitor progress ‚Üí check completion
- View results ‚Üí download ZIP
- Handle errors gracefully

**Already Tested:**
- Interactive prompts (PR #46)
- WebSocket communication (Phase 3)
- Progress monitoring (Phase 3)

---

#### ~~**Phase 5 Prompt Flow Tests**~~ ‚úÖ COMPLETE
~~**Status:** BLOCKED BY PHASE 5 NOT IMPLEMENTED~~  
~~**Priority:** Medium~~  
~~**Effort:** 4 hours~~  

**Resolution:** Completed in PR #46 with comprehensive async test suite.

---

## üìã Documentation Gaps

### ‚ùå Missing Documentation

#### **Deployment Guide**
**Status:** BASIC ONLY  
**Priority:** Medium  
**Effort:** 3 hours  
**Description:** `DASHBOARD_GUIDE.md` has basic deployment, needs production guide

**Missing Content:**
- Production deployment (Nginx, systemd)
- Docker Compose setup
- Environment variable configuration
- Monitoring and logging setup
- Backup and restore procedures

---

#### **API Reference**
**Status:** INLINE DOCS ONLY  
**Priority:** Low  
**Effort:** 2 hours  
**Description:** No comprehensive API documentation

**Proposed:**
- OpenAPI/Swagger docs
- Auto-generated from FastAPI
- Example requests/responses
- Authentication guide

---

## üìä Summary Statistics

### Tasks Deferred by Category

| Category | Deferred Tasks | Total Effort |
|----------|---------------|--------------|
| Phase 4: Results Visualization | 2 | 8 hours |
| ~~Phase 5: Interactive Prompts~~ ‚úÖ | ~~5~~ **6** (1 critical + 5 enhancements) | ~~21 hours~~ **9.25 hours** |
| Phase 2: Input Handling | 2 | 8 hours |
| Phase 3: Progress Monitoring | 2 | 6 hours |
| Enhancement Wave 3 | 5 | 19 hours |
| ~~Smart Deduplication~~ ‚úÖ | ~~2~~ **0** (COMPLETE) | ~~8 hours~~ **2 hours** (FAISS optimization remaining) |
| Dashboard v2.0 Architecture | 3 | 110+ hours |
| Testing | 1 | 4 hours (E2E tests) |
| Documentation | 2 | 5 hours |
| **TOTAL** | **~~23~~ 20 tasks** | **~~189+~~ 173+ hours** |

### Completed Since Last Analysis (PRs #46-47)

| PR | Feature | Tasks Completed | Deferred | Net Impact |
|----|---------|----------------|----------|------------|
| #46 | Phase 5: Interactive Prompts | 5 tasks | 6 new items (9.25h) | Core complete, enhancements deferred |
| #47 | Smart Dedup Enhancements | 2 tasks + 2 new features | 0 | 6 hours saved |
| **Total** | | **7 tasks** | **6 tasks** | **17.75 hours saved** |

### Priority Distribution

- üî¥ High: 1 task (5%) - pytest-asyncio dependency
- üü° Medium: 10 tasks (50%)
- üü¢ Low: 9 tasks (45%)

### Effort Distribution

- Quick wins (<5h): 13 tasks (65%)
- Medium effort (5-10h): 4 tasks (20%)
- Large effort (>10h): 3 tasks (15%)

---

## üéØ Recommended Next Actions

### Phase 0: Critical Fix (1 minute)
1. **Add pytest-asyncio to requirements-dev.txt** (HIGH PRIORITY - prevents CI failures)

### Phase 1: Quick Wins (31 hours)
1. **Task 4.3: Results Comparison View** (5h) - High user value
2. **Task 4.4: Results Summary Cards** (3h) - Improves UX
3. ~~**Smart Dedup: Model Caching** (2h)~~ ‚úÖ COMPLETE (PR #47)
4. **Evidence Decay: Integration with Gap Scoring** (4h) - Makes decay useful
5. **Evidence Decay: Field Presets** (2h) - Better UX
6. **Phase 5: Limited Prompt Types** (2h) - Complete run_mode and continue prompts
7. **Phase 5: Configurable Timeout** (1h) - Per-prompt timeout control
8. **Phase 5: Interactive Prompts Documentation** (2h) - User guide + screenshots
9. **ROI Optimizer: Cost-Aware Ordering** (2h) - Budget control
10. **Phase 2: Duplicate Detection** (3h) - Prevents user errors
11. **Phase 3: ETA Accuracy** (2h) - Better estimates
12. **Documentation: Deployment Guide** (3h) - Production readiness
13. **Documentation: API Reference** (2h) - Developer experience

### Phase 2: UX Enhancements (9 hours)
1. **Phase 5: Multi-Select Pillars** (2h) - Better pillar selection UX
2. **Phase 5: Prompt History** (3h) - Audit trail and replay
3. **E2E Dashboard Tests** (4h) - Quality assurance

### Phase 3: Performance & Scale (18 hours)
1. **Smart Dedup: FAISS Integration** (2h) - Remaining optimization for large datasets
2. **Phase 2: Better PDF Metadata** (5h)
3. **Phase 3: Progress Replay** (4h)
4. **ROI Optimizer: Dynamic Adjustment** (3h)
5. ~~**Testing: Prompt Flow Tests** (4h)~~ ‚úÖ COMPLETE (PR #46)

### Long-Term: v2.0 Architecture (110+ hours)
- Database migration
- Horizontal scaling
- Advanced authentication
- (Defer until user base grows)

---

## üìà Progress Report

### Recent Achievements (PRs #46-47)

**PR #46: Interactive Prompts** ‚úÖ
- Delivered full Phase 5 implementation (21 hours of work)
- Achieved 1:1 feature parity with terminal experience
- 6 passing tests, 4 skipped tests (expected in test environment)
- Zero breaking changes (backward compatible)

**PR #47: Smart Dedup Enhancements** ‚úÖ
- Resolved cross-batch duplicate detection limitation
- Added full metadata preservation from duplicates
- Created performance benchmarking suite
- Integrated with pipeline orchestrator (optional)
- Documented firewall/model caching issues
- Enhanced documentation (+159 lines in smart_deduplication.md)

### Impact Summary
- **Core work completed: 21 hours** (Phase 5 implementation)
- **Enhancements deferred: 9.25 hours** (6 follow-up items)
- **Net progress: 17.75 hours saved** (21h completed - 9.25h deferred + 6h from PR #47)
- **23 ‚Üí 20 remaining tasks** (13% reduction)
- **189+ ‚Üí 173+ hours remaining** (8% reduction in technical debt)

### Key Milestones Reached
1. ‚úÖ **Dashboard Feature Parity**: Web interface now equals terminal experience (core functionality)
2. ‚úÖ **Smart Deduplication Production-Ready**: Cross-batch detection + performance benchmarks
3. ‚úÖ **Async Testing Infrastructure**: pytest-asyncio configured and working
4. ‚úÖ **Pipeline Integration**: Deduplication optionally integrated with orchestrator

### Critical Action Required
‚ö†Ô∏è **Add `pytest-asyncio` to `requirements-dev.txt`** - Currently installed but not tracked, will cause CI failures

