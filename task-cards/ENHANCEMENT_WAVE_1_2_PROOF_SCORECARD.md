# Task Card: Proof Completeness Scorecard

**Task ID:** ENHANCE-W1-2  
**Wave:** 1 (Foundation & Quick Wins)  
**Priority:** CRITICAL  
**Estimated Effort:** 8 hours  
**Status:** Not Started  
**Dependencies:** None

---

## Objective

Create an interactive dashboard that answers the critical question: **"Can we publish with current evidence?"** in 30 seconds.

## Background

Current gap analysis provides completeness percentages but doesn't assess publication readiness. Researchers need a clear go/no-go decision with specific blockers and timelines.

## Success Criteria

- [ ] Researcher can answer "can we publish?" in <30 seconds
- [ ] Traffic light indicators (ðŸ”´ Red, ðŸŸ¡ Yellow, ðŸŸ¢ Green) per research goal
- [ ] Clear blocking factors identified
- [ ] Actionable next steps provided
- [ ] Timeline estimates for reaching publication readiness

## Deliverables

### 1. Core Analysis Module

**File:** `literature_review/analysis/proof_scorecard.py`

```python
"""
Proof Completeness Scorecard
Assesses publication readiness and provides strategic guidance.
"""

import json
import os
from typing import Dict, List, Tuple
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class ProofScorecardAnalyzer:
    """Analyze publication readiness across research goals."""
    
    # Publication thresholds
    TIER_1_JOURNAL_THRESHOLD = 80  # Nature, Science
    TIER_2_JOURNAL_THRESHOLD = 60  # Specialized journals
    CONFERENCE_THRESHOLD = 40      # Conference papers
    PREPRINT_THRESHOLD = 0         # arXiv (any quality)
    
    # Minimum viable completeness per pillar for system integration
    PILLAR_7_REQUIREMENTS = {
        'Pillar 1': 40,
        'Pillar 2': 50,
        'Pillar 3': 40,
        'Pillar 4': 60,
        'Pillar 5': 30,
        'Pillar 6': 40,
        'Pillar 7': 50
    }
    
    def __init__(self, gap_report: Dict, version_history: Dict, pillar_definitions: Dict):
        self.gap_report = gap_report
        self.version_history = version_history
        self.pillar_definitions = pillar_definitions
    
    def analyze(self) -> Dict:
        """Generate complete proof scorecard."""
        
        # Overall proof readiness
        overall_score = self._calculate_overall_proof_readiness()
        
        # Research goal breakdown
        research_goals = self._analyze_research_goals()
        
        # Proof requirements checklist
        proof_checklist = self._generate_proof_checklist()
        
        # Publication viability
        pub_viability = self._assess_publication_viability(overall_score)
        
        # Critical next steps
        next_steps = self._generate_critical_next_steps(research_goals)
        
        return {
            'timestamp': datetime.now().isoformat(),
            'overall_proof_status': {
                'proof_readiness_score': overall_score,
                'verdict': self._get_verdict(overall_score),
                'headline': self._generate_headline(overall_score)
            },
            'research_goals': research_goals,
            'proof_requirements_checklist': proof_checklist,
            'publication_viability': pub_viability,
            'critical_next_steps': next_steps,
            'pillar_7_readiness': self._assess_pillar_7_readiness()
        }
    
    def _calculate_overall_proof_readiness(self) -> float:
        """
        Calculate overall proof readiness (0-100).
        
        Weighted average of pillar completeness, with bonus for:
        - Evidence quality (triangulation, methodological rigor)
        - Cross-pillar integration (bio-AI bridges)
        """
        total_weighted_score = 0
        total_weight = 0
        
        for pillar_name, pillar_data in self.gap_report.items():
            if not isinstance(pillar_data, dict):
                continue
            
            # Base completeness
            completeness = pillar_data.get('average_completeness', 0)
            
            # Weight: Foundational pillars count more
            weight = 1.5 if pillar_name in ['Pillar 1', 'Pillar 3', 'Pillar 5'] else 1.0
            
            total_weighted_score += completeness * weight
            total_weight += weight
        
        avg_score = total_weighted_score / total_weight if total_weight > 0 else 0
        
        return round(avg_score, 1)
    
    def _analyze_research_goals(self) -> List[Dict]:
        """
        Analyze each major research goal.
        
        Goals:
        1. Biological mechanisms fully characterized (Pillars 1, 3, 5)
        2. AI implementations faithfully emulate biology (Pillars 2, 4, 6)
        3. Integrated system demonstrates bio-fidelity (Pillar 7)
        """
        goals = []
        
        # Goal 1: Biological Foundations
        bio_pillars = ['Pillar 1', 'Pillar 3', 'Pillar 5']
        bio_completeness = self._get_average_completeness(bio_pillars)
        bio_sufficiency = self._estimate_sufficiency(bio_pillars)
        
        goals.append({
            'goal': 'Biological stimulus-response mechanisms are fully characterized',
            'pillars': bio_pillars,
            'completeness': bio_completeness,
            'sufficiency': bio_sufficiency,
            'proof_status': self._get_proof_status(bio_sufficiency),
            'blocking_factors': self._identify_blocking_factors(bio_pillars),
            'minimum_viable': 40,
            'current_deficit': max(0, 40 - bio_completeness),
            'estimated_papers_needed': self._estimate_papers_needed(40 - bio_completeness),
            'estimated_timeline_weeks': self._estimate_timeline(40 - bio_completeness)
        })
        
        # Goal 2: AI Implementation Fidelity
        ai_pillars = ['Pillar 2', 'Pillar 4', 'Pillar 6']
        ai_completeness = self._get_average_completeness(ai_pillars)
        ai_sufficiency = self._estimate_sufficiency(ai_pillars)
        
        goals.append({
            'goal': 'AI implementations faithfully emulate biological mechanisms',
            'pillars': ai_pillars,
            'completeness': ai_completeness,
            'sufficiency': ai_sufficiency,
            'proof_status': self._get_proof_status(ai_sufficiency),
            'blocking_factors': self._identify_blocking_factors(ai_pillars),
            'dependency': 'Requires Goal 1 (biological foundations) to be proven first',
            'minimum_viable': 50,
            'current_deficit': max(0, 50 - ai_completeness),
            'estimated_papers_needed': self._estimate_papers_needed(50 - ai_completeness),
            'estimated_timeline_weeks': self._estimate_timeline(50 - ai_completeness)
        })
        
        # Goal 3: System Integration
        p7_completeness = self._get_average_completeness(['Pillar 7'])
        p7_sufficiency = self._estimate_sufficiency(['Pillar 7'])
        
        goals.append({
            'goal': 'Integrated neuromorphic system demonstrates biological fidelity and computational efficiency',
            'pillars': ['Pillar 7'],
            'completeness': p7_completeness,
            'sufficiency': p7_sufficiency,
            'proof_status': self._get_proof_status(p7_sufficiency),
            'blocking_factors': ['ALL other pillars must reach minimum thresholds first'],
            'dependency': 'Requires Goals 1 and 2',
            'minimum_viable': 50,
            'current_deficit': max(0, 50 - p7_completeness),
            'blocking_pillars': self._identify_blocking_pillars_for_p7()
        })
        
        return goals
    
    def _get_average_completeness(self, pillars: List[str]) -> float:
        """Calculate average completeness for a set of pillars."""
        total = 0
        count = 0
        
        for pillar_name in pillars:
            if pillar_name in self.gap_report:
                pillar_data = self.gap_report[pillar_name]
                if isinstance(pillar_data, dict):
                    total += pillar_data.get('average_completeness', 0)
                    count += 1
        
        return round(total / count, 1) if count > 0 else 0
    
    def _estimate_sufficiency(self, pillars: List[str]) -> float:
        """
        Estimate evidence sufficiency (quality-adjusted completeness).
        
        Simplified version - full version in Wave 2 (Evidence Sufficiency Matrix)
        """
        completeness = self._get_average_completeness(pillars)
        
        # Simple quality penalty for now
        # Assume 30% reduction due to single-source claims, lack of triangulation
        sufficiency = completeness * 0.7
        
        return round(sufficiency, 1)
    
    def _get_proof_status(self, sufficiency: float) -> str:
        """Determine proof status based on sufficiency."""
        if sufficiency >= 80:
            return 'PROVEN'
        elif sufficiency >= 60:
            return 'PROBABLE'
        elif sufficiency >= 40:
            return 'POSSIBLE'
        elif sufficiency >= 20:
            return 'INSUFFICIENT'
        else:
            return 'UNPROVEN'
    
    def _identify_blocking_factors(self, pillars: List[str]) -> List[str]:
        """Identify specific blockers preventing proof."""
        blockers = []
        
        for pillar_name in pillars:
            if pillar_name not in self.gap_report:
                continue
            
            pillar_data = self.gap_report[pillar_name]
            if not isinstance(pillar_data, dict):
                continue
            
            completeness = pillar_data.get('average_completeness', 0)
            
            if completeness < 10:
                blockers.append(f"{pillar_name} severely incomplete ({completeness:.0f}%)")
            
            # Check for 0% sub-requirements
            if 'analysis' in pillar_data:
                zero_gaps = []
                for req_key, req_data in pillar_data['analysis'].items():
                    for sub_req_key, sub_req_data in req_data.items():
                        if sub_req_data.get('completeness_percent', 0) == 0:
                            zero_gaps.append(sub_req_key)
                
                if len(zero_gaps) > 5:
                    blockers.append(f"{pillar_name}: {len(zero_gaps)} completely unfilled gaps")
        
        return blockers if blockers else ['Insufficient evidence quality and triangulation']
    
    def _estimate_papers_needed(self, deficit: float) -> int:
        """Estimate papers needed to close deficit."""
        # Assume each paper provides ~15% coverage on average
        return max(1, int(deficit / 15))
    
    def _estimate_timeline(self, deficit: float) -> int:
        """Estimate weeks to close deficit."""
        papers_needed = self._estimate_papers_needed(deficit)
        # Assume 2.5 hours per paper (search + review + judge)
        hours_needed = papers_needed * 2.5
        # Assume 10 hours research per week
        weeks = max(1, int(hours_needed / 10))
        return weeks
    
    def _generate_proof_checklist(self) -> Dict:
        """Generate proof requirements checklist."""
        total_reqs = 0
        proven = 0
        probable = 0
        possible = 0
        insufficient = 0
        unproven = 0
        
        for pillar_name, pillar_data in self.gap_report.items():
            if not isinstance(pillar_data, dict) or 'analysis' not in pillar_data:
                continue
            
            for req_key, req_data in pillar_data['analysis'].items():
                for sub_req_key, sub_req_data in req_data.items():
                    total_reqs += 1
                    
                    # Estimate sufficiency (simplified)
                    completeness = sub_req_data.get('completeness_percent', 0)
                    sufficiency = completeness * 0.7  # Quality penalty
                    
                    if sufficiency >= 80:
                        proven += 1
                    elif sufficiency >= 60:
                        probable += 1
                    elif sufficiency >= 40:
                        possible += 1
                    elif sufficiency >= 20:
                        insufficient += 1
                    else:
                        unproven += 1
        
        return {
            'total_requirements': total_reqs,
            'proven': proven,
            'probable': probable,
            'possible': possible,
            'insufficient': insufficient,
            'unproven': unproven,
            'percent_proven': round((proven / total_reqs * 100), 1) if total_reqs > 0 else 0
        }
    
    def _assess_publication_viability(self, overall_score: float) -> Dict:
        """Assess which publication venues are viable."""
        return {
            'tier_1_journal': overall_score >= self.TIER_1_JOURNAL_THRESHOLD,
            'tier_2_journal': overall_score >= self.TIER_2_JOURNAL_THRESHOLD,
            'conference_paper': 'MAYBE' if overall_score >= self.CONFERENCE_THRESHOLD else 'NO',
            'preprint': True,
            'recommended_venue': self._recommend_venue(overall_score)
        }
    
    def _recommend_venue(self, overall_score: float) -> str:
        """Recommend publication venue based on score."""
        if overall_score >= 80:
            return 'Tier 1 journal (Nature, Science, etc.)'
        elif overall_score >= 60:
            return 'Tier 2 specialized journal'
        elif overall_score >= 40:
            return 'Conference paper or workshop'
        else:
            return 'Preprint â†’ build evidence â†’ resubmit in 3-6 months'
    
    def _generate_critical_next_steps(self, research_goals: List[Dict]) -> List[str]:
        """Generate prioritized action items."""
        steps = []
        
        # Find most critical blockers
        for i, goal in enumerate(research_goals, 1):
            if goal['proof_status'] in ['UNPROVEN', 'INSUFFICIENT']:
                if 'blocking_factors' in goal and goal['blocking_factors']:
                    steps.append(
                        f"{i}. Address {goal['goal'][:50]}... - "
                        f"{goal['blocking_factors'][0]}"
                    )
        
        # Add strategic recommendations
        if len(steps) < 3:
            steps.append("3. Find independent replications for single-source claims (triangulation)")
            steps.append("4. Re-assess in 4-6 weeks after targeted search")
        
        return steps[:5]  # Top 5 only
    
    def _assess_pillar_7_readiness(self) -> Dict:
        """Assess readiness for Pillar 7 (system integration)."""
        blocking_pillars = self._identify_blocking_pillars_for_p7()
        
        current_readiness = self._get_average_completeness(['Pillar 7'])
        
        return {
            'description': 'System Integration requires ALL pillars to reach minimum thresholds',
            'current_readiness': current_readiness,
            'target_readiness': 60,
            'blocking_pillars': blocking_pillars,
            'ready_for_integration': len(blocking_pillars) == 0,
            'estimated_papers_to_readiness': sum(p['papers_needed'] for p in blocking_pillars),
            'earliest_possible_date': self._estimate_earliest_integration_date(blocking_pillars)
        }
    
    def _identify_blocking_pillars_for_p7(self) -> List[Dict]:
        """Identify pillars blocking Pillar 7 integration."""
        blockers = []
        
        for pillar_name, min_threshold in self.PILLAR_7_REQUIREMENTS.items():
            if pillar_name == 'Pillar 7':
                continue
            
            if pillar_name not in self.gap_report:
                continue
            
            pillar_data = self.gap_report[pillar_name]
            if not isinstance(pillar_data, dict):
                continue
            
            completeness = pillar_data.get('average_completeness', 0)
            
            if completeness < min_threshold:
                deficit = min_threshold - completeness
                blockers.append({
                    'pillar': pillar_name,
                    'completeness': completeness,
                    'threshold': min_threshold,
                    'deficit': deficit,
                    'papers_needed': self._estimate_papers_needed(deficit)
                })
        
        return blockers
    
    def _estimate_earliest_integration_date(self, blocking_pillars: List[Dict]) -> str:
        """Estimate when Pillar 7 integration could begin."""
        if not blocking_pillars:
            return 'NOW - All prerequisites met'
        
        total_deficit = sum(p['deficit'] for p in blocking_pillars)
        weeks = self._estimate_timeline(total_deficit)
        
        # Add buffer for comprehensive testing
        weeks += 4
        
        if weeks <= 12:
            return f'{weeks} weeks (Q1 2026)'
        elif weeks <= 26:
            return f'{weeks} weeks (Q2 2026)'
        else:
            return f'{weeks} weeks (Q3-Q4 2026)'
    
    def _get_verdict(self, score: float) -> str:
        """Get overall verdict."""
        if score >= 80:
            return 'PUBLICATION_READY'
        elif score >= 60:
            return 'NEAR_READY'
        elif score >= 40:
            return 'PROGRESS_NEEDED'
        elif score >= 20:
            return 'SIGNIFICANT_GAPS'
        else:
            return 'INSUFFICIENT_FOR_PUBLICATION'
    
    def _generate_headline(self, score: float) -> str:
        """Generate headline summary."""
        if score >= 80:
            return 'Research goals sufficiently proven for publication'
        elif score >= 60:
            return 'Close to publication readiness - address key gaps'
        elif score >= 40:
            return 'Moderate progress - targeted search needed'
        elif score >= 20:
            return 'Significant gaps remain - systematic literature search required'
        else:
            return 'Cannot prove neuromorphic framework with current evidence'


def generate_scorecard(gap_analysis_file: str, version_history_file: str, 
                      pillar_definitions_file: str, output_dir: str):
    """Generate proof scorecard and save outputs."""
    
    # Load data
    with open(gap_analysis_file, 'r') as f:
        gap_report = json.load(f)
    
    with open(version_history_file, 'r') as f:
        version_history = json.load(f)
    
    with open(pillar_definitions_file, 'r') as f:
        pillar_definitions = json.load(f)
    
    # Analyze
    analyzer = ProofScorecardAnalyzer(gap_report, version_history, pillar_definitions)
    scorecard = analyzer.analyze()
    
    # Save JSON
    os.makedirs(output_dir, exist_ok=True)
    json_output = os.path.join(output_dir, 'proof_scorecard.json')
    with open(json_output, 'w') as f:
        json.dump(scorecard, f, indent=2)
    
    logger.info(f"Saved scorecard to {json_output}")
    
    return scorecard


def main():
    """CLI entry point."""
    import sys
    
    logging.basicConfig(level=logging.INFO)
    
    scorecard = generate_scorecard(
        gap_analysis_file='gap_analysis_output/gap_analysis_report.json',
        version_history_file='review_version_history.json',
        pillar_definitions_file='pillar_definitions.json',
        output_dir='proof_scorecard_output'
    )
    
    # Print summary
    print("\n" + "="*60)
    print("PROOF COMPLETENESS SCORECARD")
    print("="*60)
    print(f"\nOverall Readiness: {scorecard['overall_proof_status']['proof_readiness_score']:.1f}%")
    print(f"Verdict: {scorecard['overall_proof_status']['verdict']}")
    print(f"\n{scorecard['overall_proof_status']['headline']}")
    print("\nPublication Viability:")
    for venue, viable in scorecard['publication_viability'].items():
        if venue != 'recommended_venue':
            print(f"  {venue}: {viable}")
    print(f"\nRecommended: {scorecard['publication_viability']['recommended_venue']}")
    print("\nCritical Next Steps:")
    for step in scorecard['critical_next_steps']:
        print(f"  {step}")
    print("\n" + "="*60)


if __name__ == '__main__':
    main()
```

### 2. HTML Visualization

**File:** `literature_review/analysis/proof_scorecard_viz.py`

```python
"""Generate interactive HTML visualization for proof scorecard."""

import json
from datetime import datetime


def generate_html(scorecard: Dict, output_file: str):
    """Generate interactive HTML dashboard."""
    
    html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proof Completeness Scorecard</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #333;
            margin-bottom: 10px;
        }}
        .timestamp {{
            color: #666;
            font-size: 14px;
            margin-bottom: 30px;
        }}
        .overall-status {{
            background: #f8f9fa;
            padding: 20px;
            border-radius: 6px;
            margin-bottom: 30px;
        }}
        .score {{
            font-size: 48px;
            font-weight: bold;
            margin: 10px 0;
        }}
        .score.low {{ color: #dc3545; }}
        .score.medium {{ color: #ffc107; }}
        .score.high {{ color: #28a745; }}
        .verdict {{
            font-size: 20px;
            margin: 10px 0;
        }}
        .headline {{
            font-size: 16px;
            color: #666;
            margin: 10px 0;
        }}
        .section {{
            margin: 30px 0;
        }}
        .section h2 {{
            color: #333;
            border-bottom: 2px solid #007bff;
            padding-bottom: 10px;
        }}
        .goal-card {{
            background: #fff;
            border: 1px solid #dee2e6;
            border-radius: 6px;
            padding: 20px;
            margin: 15px 0;
        }}
        .goal-header {{
            display: flex;
            align-items: center;
            margin-bottom: 15px;
        }}
        .status-indicator {{
            font-size: 32px;
            margin-right: 15px;
        }}
        .progress-bar {{
            background: #e9ecef;
            border-radius: 4px;
            height: 24px;
            overflow: hidden;
            margin: 10px 0;
        }}
        .progress-fill {{
            background: #28a745;
            height: 100%;
            text-align: center;
            color: white;
            font-size: 12px;
            line-height: 24px;
            transition: width 0.3s ease;
        }}
        .progress-fill.low {{ background: #dc3545; }}
        .progress-fill.medium {{ background: #ffc107; }}
        .progress-fill.high {{ background: #28a745; }}
        .metric {{
            display: inline-block;
            margin: 5px 15px 5px 0;
            font-size: 14px;
        }}
        .metric-label {{
            color: #666;
            margin-right: 5px;
        }}
        .metric-value {{
            font-weight: bold;
        }}
        .checklist {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }}
        .checklist-item {{
            background: #f8f9fa;
            padding: 15px;
            border-radius: 6px;
            text-align: center;
        }}
        .checklist-number {{
            font-size: 32px;
            font-weight: bold;
            color: #007bff;
        }}
        .checklist-label {{
            font-size: 14px;
            color: #666;
            margin-top: 5px;
        }}
        .next-steps {{
            background: #e7f3ff;
            border-left: 4px solid #007bff;
            padding: 20px;
            margin: 20px 0;
        }}
        .next-steps ol {{
            margin: 10px 0;
            padding-left: 20px;
        }}
        .next-steps li {{
            margin: 8px 0;
        }}
        .publication-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }}
        .pub-option {{
            border: 2px solid #dee2e6;
            border-radius: 6px;
            padding: 15px;
            text-align: center;
        }}
        .pub-option.viable {{
            border-color: #28a745;
            background: #d4edda;
        }}
        .pub-option.maybe {{
            border-color: #ffc107;
            background: #fff3cd;
        }}
        .pub-option.not-viable {{
            border-color: #dc3545;
            background: #f8d7da;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ðŸ“Š Proof Completeness Scorecard</h1>
        <div class="timestamp">Generated: {scorecard['timestamp']}</div>
        
        <div class="overall-status">
            <div class="score {get_score_class(scorecard['overall_proof_status']['proof_readiness_score'])}">
                {scorecard['overall_proof_status']['proof_readiness_score']:.1f}%
            </div>
            <div class="verdict">
                Verdict: {format_verdict(scorecard['overall_proof_status']['verdict'])}
            </div>
            <div class="headline">
                {scorecard['overall_proof_status']['headline']}
            </div>
        </div>
        
        <div class="section">
            <h2>Research Goals</h2>
            {generate_goals_html(scorecard['research_goals'])}
        </div>
        
        <div class="section">
            <h2>Proof Requirements Checklist</h2>
            {generate_checklist_html(scorecard['proof_requirements_checklist'])}
        </div>
        
        <div class="section">
            <h2>Publication Viability</h2>
            {generate_publication_html(scorecard['publication_viability'])}
        </div>
        
        <div class="next-steps">
            <h3>ðŸŽ¯ Critical Next Steps</h3>
            <ol>
                {generate_next_steps_html(scorecard['critical_next_steps'])}
            </ol>
        </div>
    </div>
</body>
</html>
"""
    
    with open(output_file, 'w') as f:
        f.write(html)


def get_score_class(score):
    """Get CSS class for score."""
    if score >= 60:
        return 'high'
    elif score >= 40:
        return 'medium'
    else:
        return 'low'


# ... (additional helper functions for HTML generation)
```

## Testing Plan

### Unit Tests

```python
# tests/unit/test_proof_scorecard.py

def test_calculate_overall_proof_readiness():
    """Test overall score calculation."""
    # TODO: Implement

def test_research_goal_analysis():
    """Test research goal breakdown."""
    # TODO: Implement

def test_publication_viability():
    """Test publication venue recommendations."""
    # TODO: Implement
```

### Integration Tests

```bash
# Generate scorecard with real data
python -m literature_review.analysis.proof_scorecard

# Verify outputs
ls -la proof_scorecard_output/
# Should have: proof_scorecard.json, proof_readiness.html
```

## Acceptance Criteria

- [ ] Scorecard generates without errors
- [ ] Overall proof readiness score is accurate
- [ ] Research goals correctly categorized
- [ ] Blocking factors are specific and actionable
- [ ] Publication venue recommendations are appropriate
- [ ] Timeline estimates are reasonable
- [ ] HTML visualization renders correctly
- [ ] User can make publish/wait decision in <30 seconds

## Integration Points

- Integrate into `pipeline_orchestrator.py` as final phase
- Add to gap analysis output directory
- Link from executive summary

## Notes

- Focus on clarity and actionability
- Traffic light indicators must be obvious
- Timeline estimates can be rough - will refine in Wave 2

## Related Tasks

- ENHANCE-W2-1 (Evidence Sufficiency Matrix) - Will improve sufficiency calculations
- ENHANCE-W2-2 (Proof Chain Dependencies) - Will improve blocking factor identification

---

**Created:** 2025-11-16  
**Assigned To:** TBD  
**Target Completion:** Wave 1 (Week 1)
