# Integration & E2E Testing Task Cards

**Date Created:** November 10, 2025  
**Status:** Ready for Assignment  
**Total Cards:** 8 (4 phases)  
**Estimated Total Effort:** 6-8 weeks  
**Prerequisites:** All 4 architecture task cards completed âœ…

---

## ðŸ“‹ Overview

These task cards implement the comprehensive integration and end-to-end testing strategy outlined in `INTEGRATION_E2E_TESTING_ASSESSMENT.md`. Cards are organized by implementation phase for systematic rollout.

**Benefits:**
- Prevent regressions from future changes
- Catch integration bugs before production
- Enable safe refactoring with confidence
- Document system behavior through executable tests
- Increase deployment confidence across team

**Cost Estimate:** ~$500/year for automated testing (mostly mocked, E2E on schedule)

---

## ðŸŽ« PHASE 1: FOUNDATION (Weeks 1-2)

### TASK CARD #5: Set Up Integration Test Infrastructure

**Priority:** ðŸ”´ CRITICAL  
**Estimated Effort:** 8-12 hours  
**Risk Level:** LOW  
**Dependencies:** None  
**Blocks:** All integration and E2E tests

#### Problem Statement

Integration and E2E test infrastructure needs to be established before individual tests can be implemented. This includes directory structure, test fixtures, data generators, and helper utilities.

#### Acceptance Criteria

**Success Metrics:**
- [x] Directory structure created and recognized by pytest
- [x] Test fixture framework operational
- [x] Test data generator can create synthetic papers
- [x] Helper utilities for setup/teardown working
- [x] Documentation for test patterns complete

**Technical Requirements:**
- [x] Create `tests/integration/__init__.py`
- [x] Create `tests/e2e/__init__.py`
- [x] Create `tests/fixtures/test_data_generator.py`
- [x] Create `tests/conftest.py` with shared fixtures
- [x] Update `pytest.ini` with new markers
- [x] Create `tests/INTEGRATION_TESTING_GUIDE.md`

#### Implementation Guide

**Files to Create:**

**1. Directory Structure**
```bash
tests/
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â””â”€â”€ test_*.py (test files)
â”œâ”€â”€ e2e/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â””â”€â”€ test_*.py (test files)
â””â”€â”€ fixtures/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_data_generator.py
    â”œâ”€â”€ sample_papers/
    â”œâ”€â”€ version_history_fixtures/
    â””â”€â”€ csv_fixtures/
```

**2. Test Data Generator (`tests/fixtures/test_data_generator.py`)**
```python
"""Test data generation utilities for integration and E2E tests."""

import json
import os
from typing import Dict, List
from datetime import datetime
import hashlib

class TestDataGenerator:
    """Generate synthetic test data for integration tests."""
    
    def __init__(self, fixtures_dir: str = "tests/fixtures"):
        self.fixtures_dir = fixtures_dir
        os.makedirs(fixtures_dir, exist_ok=True)
    
    def create_version_history(
        self,
        filename: str,
        num_versions: int = 1,
        claim_statuses: List[str] = None
    ) -> Dict:
        """Generate version history fixture.
        
        Args:
            filename: Paper filename
            num_versions: Number of version entries
            claim_statuses: List of claim statuses (e.g., ['pending_judge_review', 'approved'])
        
        Returns:
            Version history dictionary
        """
        if claim_statuses is None:
            claim_statuses = ['pending_judge_review']
        
        history = {filename: []}
        
        for version_num in range(num_versions):
            claims = []
            for i, status in enumerate(claim_statuses):
                claim = {
                    'claim_id': hashlib.md5(f"{filename}_{version_num}_{i}".encode()).hexdigest()[:16],
                    'pillar': 'Pillar 1: Biological Plausibility',
                    'sub_requirement': 'SR 1.1: Neuron Models',
                    'evidence': f'Test evidence for claim {i}',
                    'page_number': i + 1,
                    'status': status,
                    'reviewer_confidence': 0.85,
                    'review_timestamp': datetime.now().isoformat()
                }
                
                if status in ['approved', 'rejected']:
                    claim['judge_notes'] = f'{status.capitalize()}. Test verdict.'
                    claim['judge_timestamp'] = datetime.now().isoformat()
                
                claims.append(claim)
            
            version_entry = {
                'timestamp': datetime.now().isoformat(),
                'review': {
                    'FILENAME': filename,
                    'TITLE': f'Test Paper: {filename}',
                    'Requirement(s)': claims
                }
            }
            
            if version_num > 0:
                version_entry['changes'] = {
                    'status': 'judge_update',
                    'updated_claims': len(claims)
                }
            
            history[filename].append(version_entry)
        
        return history
    
    def create_csv_database(
        self,
        num_papers: int = 5,
        completeness_target: float = 0.7
    ) -> List[Dict]:
        """Generate CSV database fixture.
        
        Args:
            num_papers: Number of paper entries
            completeness_target: Target completeness level (0.0-1.0)
        
        Returns:
            List of paper dictionaries
        """
        papers = []
        
        for i in range(num_papers):
            filename = f"test_paper_{i+1}.pdf"
            
            # Generate approved claims based on target completeness
            num_claims = int(10 * completeness_target)
            claims = []
            
            for j in range(num_claims):
                claims.append({
                    'claim_id': hashlib.md5(f"{filename}_{j}".encode()).hexdigest()[:16],
                    'pillar': 'Pillar 1: Biological Plausibility',
                    'sub_requirement': f'SR 1.{j+1}',
                    'evidence': f'Approved evidence {j}',
                    'status': 'approved',
                    'page_number': j + 1
                })
            
            paper = {
                'FILENAME': filename,
                'TITLE': f'Test Paper {i+1}',
                'PUBLICATION_YEAR': 2020 + i,
                'Requirement(s)': json.dumps(claims),
                'CORE_DOMAIN': 'neuromorphic computing',
                'SUB_DOMAIN': 'spiking neural networks',
                'REVIEW_TIMESTAMP': datetime.now().isoformat()
            }
            
            papers.append(paper)
        
        return papers
    
    def save_version_history_fixture(self, history: Dict, fixture_name: str):
        """Save version history fixture to file."""
        fixture_path = os.path.join(
            self.fixtures_dir,
            'version_history_fixtures',
            f'{fixture_name}.json'
        )
        os.makedirs(os.path.dirname(fixture_path), exist_ok=True)
        
        with open(fixture_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
        
        return fixture_path
```

**3. Shared Fixtures (`tests/conftest.py`)**
```python
"""Shared pytest fixtures for all test types."""

import pytest
import os
import tempfile
import shutil
from tests.fixtures.test_data_generator import TestDataGenerator

@pytest.fixture
def test_data_generator():
    """Provide test data generator instance."""
    return TestDataGenerator()

@pytest.fixture
def temp_workspace(tmp_path):
    """Create temporary workspace for integration tests."""
    workspace = tmp_path / "test_workspace"
    workspace.mkdir()
    
    # Create standard directory structure
    (workspace / "Research-Papers").mkdir()
    (workspace / "gap_analysis_output").mkdir()
    
    yield workspace
    
    # Cleanup handled by tmp_path fixture

@pytest.fixture
def sample_version_history(test_data_generator):
    """Provide sample version history with mixed claim statuses."""
    return test_data_generator.create_version_history(
        filename="test_paper.pdf",
        num_versions=2,
        claim_statuses=['pending_judge_review', 'approved', 'rejected']
    )

@pytest.fixture
def sample_csv_database(test_data_generator):
    """Provide sample CSV database with 70% completeness."""
    return test_data_generator.create_csv_database(
        num_papers=10,
        completeness_target=0.7
    )
```

**4. Update `pytest.ini`**
```ini
[pytest]
markers =
    unit: Fast unit tests with no dependencies
    component: Component tests with mocks
    integration: Integration tests (multi-component, may use API)
    e2e: End-to-end tests (full pipeline, requires API)
    slow: Tests taking >5 seconds
    requires_api: Tests requiring Gemini API access

testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Coverage configuration
addopts = 
    --strict-markers
    --tb=short
    -ra
    --cov-report=term-missing
    --cov-report=html
```

**5. Integration Testing Guide (`tests/INTEGRATION_TESTING_GUIDE.md`)**
```markdown
# Integration Testing Guide

## Running Integration Tests

```bash
# Run all integration tests
pytest -m integration -v

# Run specific integration test
pytest tests/integration/test_journal_to_judge.py -v

# Run with coverage
pytest -m integration --cov=. --cov-report=html
```

## Writing Integration Tests

### Test Template
```python
import pytest
from tests.fixtures.test_data_generator import TestDataGenerator

class TestYourIntegration:
    
    @pytest.mark.integration
    def test_component_integration(self, temp_workspace, sample_version_history):
        """Test integration between components."""
        # Setup
        # ... (use fixtures)
        
        # Execute
        # ... (run integration)
        
        # Assert
        # ... (verify outcomes)
```

## Best Practices

1. **Isolate tests** - Each test should be independent
2. **Use fixtures** - Leverage pytest fixtures for setup/teardown
3. **Mock APIs** - Use mocks for Gemini API to avoid costs
4. **Clean up** - Use temp_workspace fixture for file operations
5. **Clear assertions** - Test one integration point per test
```

#### Testing Strategy

**How to Verify:**
```bash
# Test that pytest recognizes new markers
pytest --markers | grep -E "(integration|e2e)"

# Test fixture availability
pytest --fixtures | grep -E "(test_data_generator|temp_workspace)"

# Run a smoke test
pytest tests/integration/ --collect-only
```

#### Success Criteria

- [ ] All directories created and `__init__.py` files present
- [ ] `pytest --markers` shows `integration` and `e2e` markers
- [ ] `TestDataGenerator` can create version history fixtures
- [ ] `TestDataGenerator` can create CSV database fixtures
- [ ] Shared fixtures (`temp_workspace`, etc.) available in tests
- [ ] Integration testing guide documentation complete

---

### TASK CARD #6: Implement INT-001 (Journal-Reviewer â†’ Judge Flow)

**Priority:** ðŸ”´ CRITICAL  
**Estimated Effort:** 6-8 hours  
**Risk Level:** MEDIUM  
**Dependencies:** Task Card #5  
**Blocks:** E2E testing

#### Problem Statement

No integration test exists to validate the critical flow from Journal-Reviewer creating version history entries through Judge processing claims. This is the primary entry point for new papers into the system.

#### Acceptance Criteria

**Success Metrics:**
- [x] Test validates version history creation from Journal-Reviewer
- [x] Test validates Judge reads pending claims correctly
- [x] Test validates Judge updates claim statuses
- [x] Test validates version history updated with timestamps
- [x] Test passes with both approved and rejected claims

**Technical Requirements:**
- [x] Test uses realistic paper data (small PDF or text extract)
- [x] Test mocks Gemini API for cost control
- [x] Test verifies data flow through version history
- [x] Test checks all required fields present
- [x] Test validates timestamp ordering

#### Implementation Guide

**File to Create:** `tests/integration/test_journal_to_judge.py`

```python
"""Integration tests for Journal-Reviewer â†’ Judge flow."""

import pytest
import json
import os
from datetime import datetime
from unittest.mock import Mock, patch
from typing import Dict, List

# Import modules to test
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

import Judge
from Judge import load_version_history, save_version_history

class TestJournalToJudgeFlow:
    """Test the integration between Journal-Reviewer and Judge."""
    
    @pytest.mark.integration
    def test_judge_processes_pending_claims(self, temp_workspace, test_data_generator):
        """Test Judge reads and processes pending claims from version history."""
        
        # Setup: Create version history with pending claims
        version_history = test_data_generator.create_version_history(
            filename="test_paper.pdf",
            num_versions=1,
            claim_statuses=['pending_judge_review', 'pending_judge_review']
        )
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(version_history, f, indent=2)
        
        # Mock Judge's API calls to avoid costs
        mock_approved_response = {
            "verdict": "approved",
            "judge_notes": "Approved. Evidence clearly supports the requirement."
        }
        
        mock_rejected_response = {
            "verdict": "rejected", 
            "judge_notes": "Rejected. Evidence does not adequately address the requirement."
        }
        
        with patch.object(Judge.APIManager, 'cached_api_call') as mock_api:
            # Alternate between approved and rejected
            mock_api.side_effect = [mock_approved_response, mock_rejected_response]
            
            # Execute: Load and process claims using Judge functions
            history = load_version_history(str(version_history_file))
            
            # Extract pending claims (simplified version of Judge logic)
            claims_to_judge = []
            for filename, versions in history.items():
                if not versions:
                    continue
                latest = versions[-1].get('review', {})
                for claim in latest.get('Requirement(s)', []):
                    if claim.get('status') == 'pending_judge_review':
                        claim['_source_filename'] = filename
                        claims_to_judge.append(claim)
            
            # Process each claim
            for claim in claims_to_judge:
                # Simulate Judge processing
                response = mock_api()
                if response:
                    claim['status'] = response['verdict']
                    claim['judge_notes'] = response['judge_notes']
                    claim['judge_timestamp'] = datetime.now().isoformat()
            
            # Update version history
            for filename, versions in history.items():
                latest = versions[-1]
                # Update claims in place
                for i, claim in enumerate(latest['review']['Requirement(s)']):
                    # Find matching updated claim
                    updated_claim = next(
                        (c for c in claims_to_judge if c['claim_id'] == claim['claim_id']),
                        None
                    )
                    if updated_claim:
                        claim.update(updated_claim)
                
                # Add new version entry
                new_version = {
                    'timestamp': datetime.now().isoformat(),
                    'review': latest['review'],
                    'changes': {
                        'status': 'judge_update',
                        'updated_claims': len(claims_to_judge),
                        'claim_ids': [c['claim_id'] for c in claims_to_judge]
                    }
                }
                versions.append(new_version)
            
            # Save updated history
            save_version_history(str(version_history_file), history)
        
        # Assert: Verify results
        with open(version_history_file, 'r') as f:
            updated_history = json.load(f)
        
        # Check version history structure
        assert "test_paper.pdf" in updated_history
        versions = updated_history["test_paper.pdf"]
        assert len(versions) == 2  # Original + Judge update
        
        # Check latest version has updated claims
        latest_version = versions[-1]
        claims = latest_version['review']['Requirement(s)']
        assert len(claims) == 2
        
        # Verify claim statuses updated
        statuses = [c['status'] for c in claims]
        assert 'approved' in statuses
        assert 'rejected' in statuses
        
        # Verify timestamps added
        for claim in claims:
            assert 'judge_timestamp' in claim
            assert 'judge_notes' in claim
        
        # Verify version entry metadata
        assert latest_version['changes']['status'] == 'judge_update'
        assert latest_version['changes']['updated_claims'] == 2
    
    @pytest.mark.integration
    def test_version_history_preserves_original_data(self, temp_workspace, test_data_generator):
        """Test that Judge doesn't corrupt original version history data."""
        
        # Setup: Create version history with specific data
        original_history = test_data_generator.create_version_history(
            filename="preserve_test.pdf",
            num_versions=1,
            claim_statuses=['pending_judge_review']
        )
        
        # Add custom fields to verify preservation
        original_history["preserve_test.pdf"][0]['review']['CUSTOM_FIELD'] = 'test_value'
        original_claim = original_history["preserve_test.pdf"][0]['review']['Requirement(s)'][0]
        original_claim['custom_evidence_field'] = 'preserve_this'
        original_claim_id = original_claim['claim_id']
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(original_history, f, indent=2)
        
        # Execute: Simulate Judge processing
        with patch.object(Judge.APIManager, 'cached_api_call') as mock_api:
            mock_api.return_value = {
                "verdict": "approved",
                "judge_notes": "Approved. Test verdict."
            }
            
            history = load_version_history(str(version_history_file))
            
            # Update claim status (minimal Judge logic)
            for filename, versions in history.items():
                latest = versions[-1]
                for claim in latest['review']['Requirement(s)']:
                    if claim.get('status') == 'pending_judge_review':
                        claim['status'] = 'approved'
                        claim['judge_notes'] = 'Approved. Test verdict.'
                        claim['judge_timestamp'] = datetime.now().isoformat()
            
            save_version_history(str(version_history_file), history)
        
        # Assert: Verify original data preserved
        with open(version_history_file, 'r') as f:
            updated_history = json.load(f)
        
        latest_version = updated_history["preserve_test.pdf"][-1]
        
        # Check custom fields preserved
        assert latest_version['review']['CUSTOM_FIELD'] == 'test_value'
        
        updated_claim = next(
            c for c in latest_version['review']['Requirement(s)'] 
            if c['claim_id'] == original_claim_id
        )
        assert updated_claim['custom_evidence_field'] == 'preserve_this'
        assert updated_claim['status'] == 'approved'  # But status updated
```

#### Testing Strategy

**How to Test:**
```bash
# Run this specific integration test
pytest tests/integration/test_journal_to_judge.py -v

# Run with coverage
pytest tests/integration/test_journal_to_judge.py --cov=Judge --cov-report=term-missing

# Run with API mocks verification
pytest tests/integration/test_journal_to_judge.py -v -s
```

#### Success Criteria

- [ ] Test creates version history with pending claims
- [ ] Test simulates Judge processing with mocked API
- [ ] Test verifies claim statuses update correctly
- [ ] Test confirms version history structure maintained
- [ ] Test validates timestamps added
- [ ] Test preserves original data fields
- [ ] Test passes consistently (>95% pass rate)

---

### TASK CARD #7: Implement INT-003 (Version History â†’ CSV Sync)

**Priority:** ðŸ”´ CRITICAL  
**Estimated Effort:** 4-6 hours  
**Risk Level:** LOW  
**Dependencies:** Task Card #5  
**Blocks:** Orchestrator integration tests

#### Problem Statement

No integration test validates the critical sync from version history to CSV database via `sync_history_to_db.py`. This sync ensures the Orchestrator has access to approved claims for gap analysis.

#### Acceptance Criteria

**Success Metrics:**
- [x] Test validates only approved claims synced
- [x] Test validates CSV format preserved
- [x] Test validates column order maintained
- [x] Test validates JSON serialization correct
- [x] Test validates no data loss

**Technical Requirements:**
- [x] Test uses version history with mixed statuses
- [x] Test verifies sync script execution
- [x] Test checks CSV row count matches approved claims
- [x] Test validates all required columns present
- [x] Test handles edge cases (empty history, all rejected, etc.)

#### Implementation Guide

**File to Create:** `tests/integration/test_version_history_sync.py`

```python
"""Integration tests for Version History â†’ CSV sync."""

import pytest
import json
import csv
import os
import pandas as pd
from typing import Dict, List

import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# Import sync functionality (if refactored) or test script execution

class TestVersionHistorySync:
    """Test sync from version history to CSV database."""
    
    @pytest.mark.integration
    def test_sync_approved_claims_to_csv(self, temp_workspace, test_data_generator):
        """Test that sync copies only approved claims to CSV."""
        
        # Setup: Create version history with mixed statuses
        version_history = {
            "paper1.pdf": [
                {
                    'timestamp': '2025-11-10T10:00:00',
                    'review': {
                        'FILENAME': 'paper1.pdf',
                        'TITLE': 'Test Paper 1',
                        'PUBLICATION_YEAR': 2024,
                        'Requirement(s)': [
                            {
                                'claim_id': 'approved_claim_1',
                                'status': 'approved',
                                'pillar': 'Pillar 1',
                                'sub_requirement': 'SR 1.1',
                                'evidence': 'Approved evidence',
                                'page_number': 1
                            },
                            {
                                'claim_id': 'rejected_claim_1',
                                'status': 'rejected',
                                'pillar': 'Pillar 1',
                                'sub_requirement': 'SR 1.2',
                                'evidence': 'Rejected evidence',
                                'page_number': 2
                            },
                            {
                                'claim_id': 'pending_claim_1',
                                'status': 'pending_judge_review',
                                'pillar': 'Pillar 1',
                                'sub_requirement': 'SR 1.3',
                                'evidence': 'Pending evidence',
                                'page_number': 3
                            }
                        ]
                    }
                }
            ],
            "paper2.pdf": [
                {
                    'timestamp': '2025-11-10T11:00:00',
                    'review': {
                        'FILENAME': 'paper2.pdf',
                        'TITLE': 'Test Paper 2',
                        'PUBLICATION_YEAR': 2025,
                        'Requirement(s)': [
                            {
                                'claim_id': 'approved_claim_2',
                                'status': 'approved',
                                'pillar': 'Pillar 2',
                                'sub_requirement': 'SR 2.1',
                                'evidence': 'Approved evidence 2',
                                'page_number': 5
                            }
                        ]
                    }
                }
            ]
        }
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(version_history, f, indent=2)
        
        csv_file = temp_workspace / "neuromorphic-research_database.csv"
        
        # Execute: Perform sync (simplified sync logic)
        # In real implementation, this would call sync_history_to_db.py
        synced_papers = []
        
        for filename, versions in version_history.items():
            if not versions:
                continue
            
            latest = versions[-1]['review']
            
            # Extract approved claims
            approved_claims = [
                c for c in latest.get('Requirement(s)', [])
                if c.get('status') == 'approved'
            ]
            
            if approved_claims:
                paper_entry = {
                    'FILENAME': latest.get('FILENAME', filename),
                    'TITLE': latest.get('TITLE', ''),
                    'PUBLICATION_YEAR': latest.get('PUBLICATION_YEAR', ''),
                    'Requirement(s)': json.dumps(approved_claims)
                }
                synced_papers.append(paper_entry)
        
        # Write to CSV
        if synced_papers:
            fieldnames = ['FILENAME', 'TITLE', 'PUBLICATION_YEAR', 'Requirement(s)']
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
                writer.writeheader()
                writer.writerows(synced_papers)
        
        # Assert: Verify sync results
        assert csv_file.exists()
        
        # Read CSV
        df = pd.read_csv(csv_file)
        
        # Check row count (should be 2 papers)
        assert len(df) == 2
        
        # Check that only approved claims are present
        for _, row in df.iterrows():
            claims = json.loads(row['Requirement(s)'])
            for claim in claims:
                assert claim['status'] == 'approved'
        
        # Verify specific counts
        paper1_row = df[df['FILENAME'] == 'paper1.pdf'].iloc[0]
        paper1_claims = json.loads(paper1_row['Requirement(s)'])
        assert len(paper1_claims) == 1  # Only 1 approved out of 3
        
        paper2_row = df[df['FILENAME'] == 'paper2.pdf'].iloc[0]
        paper2_claims = json.loads(paper2_row['Requirement(s)'])
        assert len(paper2_claims) == 1  # 1 approved claim
    
    @pytest.mark.integration
    def test_sync_handles_empty_version_history(self, temp_workspace):
        """Test sync gracefully handles empty version history."""
        
        # Setup: Empty version history
        version_history = {}
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(version_history, f, indent=2)
        
        csv_file = temp_workspace / "neuromorphic-research_database.csv"
        
        # Execute: Sync should create empty or minimal CSV
        synced_papers = []
        for filename, versions in version_history.items():
            pass  # No papers to sync
        
        # Should not create CSV or create empty CSV
        # For this test, we verify no crash occurs
        assert True  # Sync completed without error
    
    @pytest.mark.integration
    def test_sync_preserves_claim_fields(self, temp_workspace):
        """Test that all claim fields are preserved in sync."""
        
        # Setup: Version history with extended claim fields
        version_history = {
            "detailed_paper.pdf": [
                {
                    'timestamp': '2025-11-10T12:00:00',
                    'review': {
                        'FILENAME': 'detailed_paper.pdf',
                        'TITLE': 'Detailed Test',
                        'Requirement(s)': [
                            {
                                'claim_id': 'detailed_claim',
                                'status': 'approved',
                                'pillar': 'Pillar 1',
                                'sub_requirement': 'SR 1.1',
                                'evidence': 'Detailed evidence',
                                'page_number': 10,
                                'reviewer_confidence': 0.95,
                                'review_timestamp': '2025-11-10T09:00:00',
                                'source': 'deep_coverage',
                                'judge_notes': 'Approved. Excellent evidence.',
                                'judge_timestamp': '2025-11-10T10:00:00'
                            }
                        ]
                    }
                }
            ]
        }
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(version_history, f, indent=2)
        
        csv_file = temp_workspace / "neuromorphic-research_database.csv"
        
        # Execute sync
        synced_papers = []
        for filename, versions in version_history.items():
            latest = versions[-1]['review']
            approved_claims = [
                c for c in latest.get('Requirement(s)', [])
                if c.get('status') == 'approved'
            ]
            if approved_claims:
                paper_entry = {
                    'FILENAME': latest['FILENAME'],
                    'TITLE': latest['TITLE'],
                    'Requirement(s)': json.dumps(approved_claims)
                }
                synced_papers.append(paper_entry)
        
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(
                f,
                fieldnames=['FILENAME', 'TITLE', 'Requirement(s)'],
                quoting=csv.QUOTE_ALL
            )
            writer.writeheader()
            writer.writerows(synced_papers)
        
        # Assert: Verify all fields preserved
        df = pd.read_csv(csv_file)
        row = df.iloc[0]
        claims = json.loads(row['Requirement(s)'])
        claim = claims[0]
        
        # Check extended fields present
        assert 'claim_id' in claim
        assert 'reviewer_confidence' in claim
        assert claim['reviewer_confidence'] == 0.95
        assert 'review_timestamp' in claim
        assert 'source' in claim
        assert claim['source'] == 'deep_coverage'
        assert 'judge_notes' in claim
        assert 'judge_timestamp' in claim
```

#### Testing Strategy

**How to Test:**
```bash
# Run sync integration test
pytest tests/integration/test_version_history_sync.py -v

# Test with actual sync_history_to_db.py script
pytest tests/integration/test_version_history_sync.py -v --run-scripts
```

#### Success Criteria

- [ ] Test validates approved claims synced
- [ ] Test confirms rejected/pending claims excluded
- [ ] Test verifies CSV format maintained
- [ ] Test checks claim field preservation
- [ ] Test handles empty version history
- [ ] Test passes consistently

---

## ðŸŽ« PHASE 2: CORE FLOWS (Weeks 3-4)

### TASK CARD #8: Implement INT-002 (Judge DRA Appeal Flow)

**Priority:** ðŸ”´ CRITICAL  
**Estimated Effort:** 10-12 hours  
**Risk Level:** HIGH  
**Dependencies:** Task Cards #5, #6  
**Blocks:** Full pipeline E2E test

#### Problem Statement

The Judge â†’ DRA â†’ Re-Judge appeal flow is complex and critical but has no integration test. This flow determines whether rejected claims can be salvaged through deeper analysis.

#### Acceptance Criteria

**Success Metrics:**
- [x] Test validates rejected claims trigger DRA
- [x] Test validates DRA creates new claims
- [x] Test validates re-judgment of DRA claims
- [x] Test validates approval rate improvement (>60%)
- [x] Test validates version history tracks appeal process

**Technical Requirements:**
- [x] Test mocks both initial judgment and re-judgment API calls
- [x] Test simulates DRA paper re-analysis
- [x] Test verifies new claims have enhanced evidence
- [x] Test checks version history has DRA appeal entries
- [x] Test validates claim IDs are unique

#### Implementation Guide

**File to Create:** `tests/integration/test_judge_dra_flow.py`

```python
"""Integration tests for Judge â†’ DRA â†’ Re-Judge appeal flow."""

import pytest
import json
import os
from datetime import datetime
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, List

import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

import Judge
import DeepRequirementsAnalyzer as dra

class TestJudgeDRAFlow:
    """Test the Judge DRA appeal integration."""
    
    @pytest.mark.integration
    def test_rejected_claims_trigger_dra_appeal(
        self,
        temp_workspace,
        test_data_generator
    ):
        """Test that rejected claims trigger DRA and result in new claims."""
        
        # Setup: Version history with claims that will be rejected
        version_history = test_data_generator.create_version_history(
            filename="appeal_test.pdf",
            num_versions=1,
            claim_statuses=['pending_judge_review', 'pending_judge_review']
        )
        
        version_history_file = temp_workspace / "review_version_history.json"
        with open(version_history_file, 'w') as f:
            json.dump(version_history, f, indent=2)
        
        # Create fake paper file for DRA
        paper_file = temp_workspace / "Research-Papers" / "appeal_test.pdf"
        paper_file.parent.mkdir(parents=True, exist_ok=True)
        paper_file.write_text("Fake PDF content for testing DRA")
        
        # Mock Judge API - initial rejection
        mock_reject_response = {
            "verdict": "rejected",
            "judge_notes": "Rejected. Evidence is too vague and does not specifically address the requirement."
        }
        
        # Mock DRA to create new improved claim
        mock_dra_claim = {
            'claim_id': 'dra_appeal_claim_001',
            'pillar': 'Pillar 1: Biological Plausibility',
            'sub_requirement': 'SR 1.1: Neuron Models',
            'evidence': 'Enhanced evidence from DRA: Specific quote addressing the requirement.',
            'page_number': 5,
            'status': 'pending_judge_review',
            'source': 'dra_appeal',
            'filename': 'appeal_test.pdf'
        }
        
        # Mock Judge API - re-judgment approval
        mock_approve_response = {
            "verdict": "approved",
            "judge_notes": "Approved. Enhanced evidence from DRA adequately addresses the requirement."
        }
        
        with patch.object(Judge.APIManager, 'cached_api_call') as mock_judge_api, \
             patch.object(dra, 'run_analysis') as mock_dra_run:
            
            # First 2 calls: reject initial claims
            # Next 1 call: approve DRA claim
            mock_judge_api.side_effect = [
                mock_reject_response,
                mock_reject_response,
                mock_approve_response
            ]
            
            # DRA returns 1 new improved claim
            mock_dra_run.return_value = [mock_dra_claim]
            
            # Execute: Simulate Judge workflow
            history = Judge.load_version_history(str(version_history_file))
            
            # Phase 1: Initial judgment (simplified)
            rejected_claims = []
            for filename, versions in history.items():
                latest = versions[-1]
                for claim in latest['review']['Requirement(s)']:
                    if claim.get('status') == 'pending_judge_review':
                        # Judge claim
                        response = mock_judge_api()
                        claim['status'] = response['verdict']
                        claim['judge_notes'] = response['judge_notes']
                        claim['judge_timestamp'] = datetime.now().isoformat()
                        
                        if response['verdict'] == 'rejected':
                            rejected_claims.append(claim)
            
            # Phase 2: DRA appeal
            if rejected_claims:
                api_manager = Mock()  # Mock API manager
                papers_folder = str(temp_workspace / "Research-Papers")
                
                new_claims = mock_dra_run(rejected_claims, api_manager, papers_folder)
                
                # Phase 3: Re-judge DRA claims
                for new_claim in new_claims:
                    response = mock_judge_api()
                    new_claim['status'] = response['verdict']
                    new_claim['judge_notes'] = response['judge_notes']
                    new_claim['judge_timestamp'] = datetime.now().isoformat()
                
                # Add new claims to version history
                for filename, versions in history.items():
                    latest = versions[-1]
                    latest['review']['Requirement(s)'].extend(new_claims)
                    
                    # Add version entry for DRA appeal
                    new_version = {
                        'timestamp': datetime.now().isoformat(),
                        'review': latest['review'],
                        'changes': {
                            'status': 'dra_appeal',
                            'new_claims': len(new_claims),
                            'claim_ids': [c['claim_id'] for c in new_claims]
                        }
                    }
                    versions.append(new_version)
            
            Judge.save_version_history(str(version_history_file), history)
        
        # Assert: Verify DRA appeal workflow
        with open(version_history_file, 'r') as f:
            final_history = json.load(f)
        
        versions = final_history["appeal_test.pdf"]
        
        # Should have original + DRA appeal version
        assert len(versions) >= 2
        
        # Check final version has all claims
        final_version = versions[-1]
        all_claims = final_version['review']['Requirement(s)']
        
        # Should have 2 original + 1 DRA claim = 3 total
        assert len(all_claims) == 3
        
        # Check statuses
        statuses = [c['status'] for c in all_claims]
        assert statuses.count('rejected') == 2  # Original claims
        assert statuses.count('approved') == 1  # DRA claim
        
        # Verify DRA claim has source field
        dra_claim = next(c for c in all_claims if c.get('source') == 'dra_appeal')
        assert dra_claim['status'] == 'approved'
        assert 'Enhanced evidence from DRA' in dra_claim['evidence']
        
        # Verify version history metadata
        assert final_version['changes']['status'] == 'dra_appeal'
        assert final_version['changes']['new_claims'] == 1
    
    @pytest.mark.integration
    def test_dra_approval_rate_improvement(self, temp_workspace):
        """Test that DRA improves approval rate of rejected claims."""
        
        # This test validates the core value proposition of DRA
        # Initial rejection rate: 100%
        # Post-DRA approval rate: Should be >60%
        
        # Setup: 10 claims that will initially be rejected
        initial_claims = []
        for i in range(10):
            initial_claims.append({
                'claim_id': f'initial_claim_{i}',
                'status': 'pending_judge_review',
                'pillar': 'Pillar 1',
                'sub_requirement': f'SR 1.{i}',
                'evidence': f'Weak evidence {i}',
                'page_number': i + 1
            })
        
        # Mock: All initially rejected
        # Then 7 out of 10 DRA appeals approved (70% success)
        dra_claims = []
        for i in range(10):
            dra_claims.append({
                'claim_id': f'dra_claim_{i}',
                'status': 'pending_judge_review',
                'pillar': 'Pillar 1',
                'sub_requirement': f'SR 1.{i}',
                'evidence': f'Enhanced DRA evidence {i}',
                'page_number': i + 1,
                'source': 'dra_appeal'
            })
        
        with patch.object(Judge.APIManager, 'cached_api_call') as mock_api, \
             patch.object(dra, 'run_analysis') as mock_dra:
            
            # Initial: All rejected
            initial_responses = [
                {"verdict": "rejected", "judge_notes": "Rejected. Weak evidence."}
                for _ in range(10)
            ]
            
            # DRA re-judgment: 7 approved, 3 rejected (70% success)
            dra_responses = [
                {"verdict": "approved" if i < 7 else "rejected", 
                 "judge_notes": f"{'Approved' if i < 7 else 'Rejected'}. DRA evidence."}
                for i in range(10)
            ]
            
            mock_api.side_effect = initial_responses + dra_responses
            mock_dra.return_value = dra_claims
            
            # Execute: Simulate full Judge + DRA workflow
            rejected_count = 0
            for claim in initial_claims:
                response = mock_api()
                claim['status'] = response['verdict']
                if response['verdict'] == 'rejected':
                    rejected_count += 1
            
            # Trigger DRA
            api_manager = Mock()
            new_claims = mock_dra(initial_claims, api_manager, "fake_path")
            
            # Re-judge
            approved_on_appeal = 0
            for claim in new_claims:
                response = mock_api()
                claim['status'] = response['verdict']
                if response['verdict'] == 'approved':
                    approved_on_appeal += 1
            
            # Calculate approval rate
            approval_rate = approved_on_appeal / len(new_claims)
        
        # Assert: DRA improves approval rate
        assert rejected_count == 10  # All initially rejected
        assert approved_on_appeal == 7  # 70% approved on appeal
        assert approval_rate == 0.7  # 70% success rate
        assert approval_rate > 0.6  # Exceeds 60% threshold
```

**Estimated Lines:** ~400-500  
**Complexity:** High (multi-phase workflow, multiple mocks)

#### Success Criteria

- [ ] Test simulates initial rejection of claims
- [ ] Test triggers DRA appeal process
- [ ] Test validates DRA creates new claims
- [ ] Test re-judges DRA claims with mocked API
- [ ] Test confirms >60% approval rate improvement
- [ ] Test verifies version history tracks appeal flow

---

### TASK CARD #9: Implement INT-004 & INT-005

**Priority:** ðŸŸ¡ HIGH  
**Estimated Effort:** 8-10 hours  
**Risk Level:** MEDIUM  
**Dependencies:** Task Cards #5, #7  
**Blocks:** E2E convergence testing

#### Problem Statement

Orchestrator gap analysis and Deep-Reviewer targeted analysis need integration tests to validate the closed-loop improvement cycle.

**Note:** This card combines INT-004 and INT-005 due to their tight coupling in the iterative workflow.

#### Acceptance Criteria

**INT-004 (Orchestrator Gap Analysis):**
- [x] Test validates gap identification from CSV
- [x] Test validates completeness score calculation
- [x] Test validates direction JSON generation
- [x] Test validates priority ordering

**INT-005 (Deep-Reviewer Targeted):**
- [x] Test validates direction reading
- [x] Test validates targeted paper selection
- [x] Test validates new claim creation
- [x] Test validates version history update

#### Implementation Guide

**File to Create:** `tests/integration/test_orchestrator_deep_reviewer.py`

*(Implementation details similar to previous tests, ~400-500 lines)*

---

## ðŸŽ« PHASE 3: E2E TESTS (Weeks 5-6)

### TASK CARD #10: Implement E2E-001 (Full Pipeline - Single Paper)

**Priority:** ðŸ”´ CRITICAL  
**Estimated Effort:** 12-16 hours  
**Risk Level:** HIGH  
**Dependencies:** Task Cards #5-9  
**Blocks:** Production deployment confidence

#### Problem Statement

No end-to-end test validates the complete pipeline from paper ingestion through gap closure. This is the ultimate validation of system integration.

#### Acceptance Criteria

**Success Metrics:**
- [x] Test processes real paper through full pipeline
- [x] Test validates each stage output
- [x] Test confirms convergence achieved
- [x] Test verifies all reports generated
- [x] Test execution time <30 minutes (with mocked APIs)

**Technical Requirements:**
- [x] Test uses realistic test paper (small PDF)
- [x] Test mocks Gemini API calls
- [x] Test validates data flow through all components
- [x] Test checks output file existence and format
- [x] Test measures and reports performance metrics

#### Implementation Guide

**File to Create:** `tests/e2e/test_full_pipeline.py`

*(Full E2E test implementation, ~600-800 lines)*

---

### TASK CARD #11: Implement E2E-002 (Iterative Convergence Loop)

**Priority:** ðŸŸ¡ HIGH  
**Estimated Effort:** 10-12 hours  
**Risk Level:** HIGH  
**Dependencies:** Task Card #10  
**Blocks:** Production deployment

#### Problem Statement

The iterative Deep-Reviewer loop that drives convergence to 100% coverage needs validation but is complex and time-consuming to test.

#### Acceptance Criteria

**Success Metrics:**
- [x] Test validates multi-iteration convergence
- [x] Test confirms <5% threshold triggers termination
- [x] Test validates score history tracking
- [x] Test verifies iteration limit respected
- [x] Test execution time <60 minutes (mocked)

*(Full implementation details similar to E2E-001)*

---

## ðŸŽ« PHASE 4: CI/CD & POLISH (Weeks 7-8)

### TASK CARD #12: Set Up GitHub Actions CI/CD

**Priority:** ðŸŸ¢ MEDIUM  
**Estimated Effort:** 6-8 hours  
**Risk Level:** LOW  
**Dependencies:** Task Cards #5-11  
**Blocks:** Automated testing

#### Problem Statement

Integration and E2E tests exist but are not automated in CI/CD pipeline. Manual test execution is error-prone and time-consuming.

#### Acceptance Criteria

**Success Metrics:**
- [x] GitHub Actions workflow configured
- [x] Unit tests run on every commit
- [x] Integration tests run on every PR
- [x] E2E tests run nightly
- [x] Coverage reports published

**Technical Requirements:**
- [x] Create `.github/workflows/integration-tests.yml`
- [x] Create `.github/workflows/e2e-tests.yml`
- [x] Configure secrets for API keys
- [x] Set up coverage reporting (Codecov)
- [x] Add status badges to README

#### Implementation Guide

**File to Create:** `.github/workflows/integration-tests.yml`

```yaml
name: Integration Tests

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  integration-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Run unit tests
        run: pytest -m unit -v --cov=. --cov-report=xml
      
      - name: Run component tests
        run: pytest -m component -v --cov=. --cov-append --cov-report=xml
      
      - name: Run integration tests
        run: pytest -m integration -v --cov=. --cov-append --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          flags: integration
          name: integration-coverage
```

**File to Create:** `.github/workflows/e2e-tests.yml`

```yaml
name: E2E Tests (Nightly)

on:
  schedule:
    - cron: '0 2 * * *'  # Run at 2 AM daily
  workflow_dispatch:  # Allow manual trigger

jobs:
  e2e-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
      
      - name: Run E2E tests
        run: pytest -m e2e -v --timeout=3600
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      
      - name: Upload E2E test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-test-results
          path: |
            gap_analysis_output/
            generated_plots/
            *.log
```

#### Success Criteria

- [ ] Workflows run successfully on test PR
- [ ] Coverage reports generated and uploaded
- [ ] E2E tests can be triggered manually
- [ ] Status badges added to README
- [ ] Team notified of test failures

---

## ðŸ“Š Summary

**Total Task Cards:** 8  
**Total Estimated Effort:** 66-92 hours (~6-8 weeks)  
**Priority Breakdown:**
- ðŸ”´ Critical: 5 cards
- ðŸŸ¡ High: 2 cards
- ðŸŸ¢ Medium: 1 card

**Success Metrics:**
- >80% integration test coverage
- 100% E2E coverage of primary workflows
- >98% test pass rate
- Automated CI/CD pipeline
- Comprehensive test documentation

**Next Steps:**
1. Review and approve these task cards
2. Assign cards to team members
3. Begin Phase 1: Foundation (Task Cards #5-7)
4. Track progress weekly
5. Adjust timeline based on learnings

---

**Document Version:** 1.0  
**Last Updated:** November 10, 2025  
**Status:** âœ… Ready for Assignment
